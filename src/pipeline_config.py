# src/pipeline_config.py
"""
Pipeline YAML é…ç½®è§£æå™¨

å¤„ç† pipeline YAML æ–‡ä»¶çš„åŠ è½½ã€éªŒè¯å’Œç®¡ç†ï¼Œ
åŒ…æ‹¬ schema éªŒè¯ã€å¼•ç”¨å®Œæ•´æ€§æ£€æŸ¥å’Œå¾ªç¯ä¾èµ–æ£€æµ‹ã€‚
"""

from __future__ import annotations

import yaml
from pathlib import Path
from typing import Dict, List, Any, Optional, Set
import logging

from .models import PipelineConfig
from .agent_registry import load_agent, list_available_agents, AgentConfig
from .error_handler import create_config_error, create_data_error, handle_error

logger = logging.getLogger(__name__)

# è·å–é¡¹ç›®æ ¹ç›®å½•
ROOT_DIR = Path(__file__).resolve().parent.parent
PIPELINES_DIR = ROOT_DIR / "pipelines"


class PipelineConfigError(Exception):
    """Pipeline é…ç½®é”™è¯¯ï¼ˆå‘åå…¼å®¹ï¼‰"""
    pass


class PipelineValidator:
    """Pipeline é…ç½®éªŒè¯å™¨"""
    
    def __init__(self):
        self.available_agents: Set[str] = set()
        self.agent_flows: Dict[str, Set[str]] = {}
        self._load_agent_info()
    
    def detect_circular_dependencies(self, config: PipelineConfig) -> List[str]:
        """
        æ£€æµ‹ Pipeline æ­¥éª¤é—´çš„å¾ªç¯ä¾èµ–
        
        Args:
            config: Pipeline é…ç½®
        
        Returns:
            é”™è¯¯åˆ—è¡¨ï¼ŒåŒ…å«å¾ªç¯ä¾èµ–çš„è¯¦ç»†ä¿¡æ¯
        """
        errors = []
        
        # æ„å»ºä¾èµ–å›¾
        dependencies: Dict[str, Set[str]] = {}
        step_outputs: Dict[str, str] = {}
        
        for step in config.steps:
            dependencies[step.id] = set()
            step_outputs[step.output_key] = step.id
        
        # åˆ†æè¾“å…¥æ˜ å°„ä¸­çš„ä¾èµ–å…³ç³»
        for step in config.steps:
            for param, source in step.input_mapping.items():
                # å¦‚æœæºæ˜¯å…¶ä»–æ­¥éª¤çš„è¾“å‡º
                if source in step_outputs:
                    source_step = step_outputs[source]
                    if source_step != step.id:  # ä¸èƒ½ä¾èµ–è‡ªå·±
                        dependencies[step.id].add(source_step)
                    else:
                        errors.append(
                            f"æ­¥éª¤ '{step.id}' ä¸èƒ½ä¾èµ–è‡ªå·±çš„è¾“å‡º\n"
                            f"  é—®é¢˜: input_mapping['{param}'] = '{source}'\n"
                            f"  ä¿®å¤å»ºè®®: è¯·ç§»é™¤æ­¤è‡ªå¼•ç”¨ï¼Œæˆ–ä½¿ç”¨å…¶ä»–æ­¥éª¤çš„è¾“å‡º"
                        )
        
        # æ£€æµ‹å¾ªç¯ä¾èµ–
        def find_cycle_path(node: str, visited: Set[str], rec_stack: List[str]) -> Optional[List[str]]:
            """æŸ¥æ‰¾å¾ªç¯ä¾èµ–è·¯å¾„"""
            visited.add(node)
            rec_stack.append(node)
            
            for neighbor in dependencies.get(node, set()):
                if neighbor not in visited:
                    cycle = find_cycle_path(neighbor, visited, rec_stack)
                    if cycle:
                        return cycle
                elif neighbor in rec_stack:
                    # æ‰¾åˆ°å¾ªç¯ï¼Œè¿”å›å¾ªç¯è·¯å¾„
                    cycle_start = rec_stack.index(neighbor)
                    return rec_stack[cycle_start:] + [neighbor]
            
            rec_stack.pop()
            return None
        
        visited: Set[str] = set()
        for step_id in dependencies:
            if step_id not in visited:
                cycle_path = find_cycle_path(step_id, visited, [])
                if cycle_path:
                    cycle_str = " -> ".join(cycle_path)
                    errors.append(
                        f"æ£€æµ‹åˆ°å¾ªç¯ä¾èµ–:\n"
                        f"  å¾ªç¯è·¯å¾„: {cycle_str}\n"
                        f"  ä¿®å¤å»ºè®®: è¯·é‡æ–°è®¾è®¡æ­¥éª¤é—´çš„æ•°æ®æµï¼Œæ‰“ç ´å¾ªç¯ä¾èµ–"
                    )
                    break  # åªæŠ¥å‘Šç¬¬ä¸€ä¸ªå¾ªç¯
        
        return errors
    
    def _load_agent_info(self):
        """åŠ è½½å¯ç”¨çš„ agent å’Œ flow ä¿¡æ¯"""
        try:
            self.available_agents = set(list_available_agents())
            
            for agent_id in self.available_agents:
                try:
                    agent = load_agent(agent_id)
                    self.agent_flows[agent_id] = {flow.name for flow in agent.flows}
                except Exception as e:
                    logger.warning(f"æ— æ³•åŠ è½½ agent {agent_id} çš„é…ç½®: {e}")
                    self.agent_flows[agent_id] = set()
                    
        except Exception as e:
            logger.warning(f"åŠ è½½ agent ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            self.available_agents = set()
            self.agent_flows = {}
    
    def validate_references(self, config: PipelineConfig) -> List[str]:
        """éªŒè¯é…ç½®ä¸­çš„å¼•ç”¨å®Œæ•´æ€§ï¼Œæä¾›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œä¿®å¤å»ºè®®"""
        errors = []
        
        # é¦–å…ˆæ£€æµ‹å¾ªç¯ä¾èµ–
        cycle_errors = self.detect_circular_dependencies(config)
        errors.extend(cycle_errors)
        
        # éªŒè¯æ­¥éª¤ä¸­çš„ agent å’Œ flow å¼•ç”¨
        for step in config.steps:
            # æ£€æŸ¥ agent æ˜¯å¦å­˜åœ¨
            if step.agent not in self.available_agents:
                available_list = ", ".join(sorted(self.available_agents)) if self.available_agents else "æ— "
                errors.append(
                    f"æ­¥éª¤ '{step.id}' å¼•ç”¨äº†ä¸å­˜åœ¨çš„ agent: {step.agent}\n"
                    f"  å¯ç”¨çš„ agents: {available_list}\n"
                    f"  ä¿®å¤å»ºè®®: è¯·æ£€æŸ¥ agent ID çš„æ‹¼å†™ï¼Œæˆ–åˆ›å»ºæ–°çš„ agent"
                )
                continue
                
            # æ£€æŸ¥ flow æ˜¯å¦å­˜åœ¨
            agent_flows = self.agent_flows.get(step.agent, set())
            if step.flow not in agent_flows:
                available_flows = ", ".join(sorted(agent_flows)) if agent_flows else "æ— "
                errors.append(
                    f"æ­¥éª¤ '{step.id}' å¼•ç”¨äº† agent '{step.agent}' ä¸­ä¸å­˜åœ¨çš„ flow: {step.flow}\n"
                    f"  å¯ç”¨çš„ flows: {available_flows}\n"
                    f"  ä¿®å¤å»ºè®®: è¯·æ£€æŸ¥ flow åç§°çš„æ‹¼å†™ï¼Œæˆ–åœ¨ agent '{step.agent}' ä¸­åˆ›å»ºæ–°çš„ flow"
                )
        
        # éªŒè¯ baseline ä¸­çš„ flow å¼•ç”¨
        if config.baseline:
            for step_id, baseline_step in config.baseline.steps.items():
                # æ‰¾åˆ°å¯¹åº”çš„ pipeline æ­¥éª¤
                pipeline_step = None
                for step in config.steps:
                    if step.id == step_id:
                        pipeline_step = step
                        break
                
                if not pipeline_step:
                    step_ids = ", ".join([s.id for s in config.steps])
                    errors.append(
                        f"Baseline å¼•ç”¨äº†ä¸å­˜åœ¨çš„æ­¥éª¤: {step_id}\n"
                        f"  å¯ç”¨çš„æ­¥éª¤ IDs: {step_ids}\n"
                        f"  ä¿®å¤å»ºè®®: è¯·æ£€æŸ¥æ­¥éª¤ ID çš„æ‹¼å†™ï¼Œæˆ–ä» baseline ä¸­ç§»é™¤æ­¤æ­¥éª¤"
                    )
                elif pipeline_step:
                    agent_flows = self.agent_flows.get(pipeline_step.agent, set())
                    if baseline_step.flow not in agent_flows:
                        available_flows = ", ".join(sorted(agent_flows)) if agent_flows else "æ— "
                        errors.append(
                            f"Baseline æ­¥éª¤ '{step_id}' å¼•ç”¨äº† agent '{pipeline_step.agent}' ä¸­ä¸å­˜åœ¨çš„ flow: {baseline_step.flow}\n"
                            f"  å¯ç”¨çš„ flows: {available_flows}\n"
                            f"  ä¿®å¤å»ºè®®: è¯·æ£€æŸ¥ flow åç§°çš„æ‹¼å†™ï¼Œæˆ–åœ¨ agent '{pipeline_step.agent}' ä¸­åˆ›å»ºæ–°çš„ flow"
                        )
        
        # éªŒè¯å˜ä½“ä¸­çš„ flow å¼•ç”¨
        for variant_name, variant in config.variants.items():
            for step_id, override in variant.overrides.items():
                # æ‰¾åˆ°å¯¹åº”çš„ pipeline æ­¥éª¤
                pipeline_step = None
                for step in config.steps:
                    if step.id == step_id:
                        pipeline_step = step
                        break
                
                if not pipeline_step:
                    step_ids = ", ".join([s.id for s in config.steps])
                    errors.append(
                        f"å˜ä½“ '{variant_name}' å¼•ç”¨äº†ä¸å­˜åœ¨çš„æ­¥éª¤: {step_id}\n"
                        f"  å¯ç”¨çš„æ­¥éª¤ IDs: {step_ids}\n"
                        f"  ä¿®å¤å»ºè®®: è¯·æ£€æŸ¥æ­¥éª¤ ID çš„æ‹¼å†™ï¼Œæˆ–ä»å˜ä½“ä¸­ç§»é™¤æ­¤è¦†ç›–"
                    )
                elif override.flow:
                    agent_flows = self.agent_flows.get(pipeline_step.agent, set())
                    if override.flow not in agent_flows:
                        available_flows = ", ".join(sorted(agent_flows)) if agent_flows else "æ— "
                        errors.append(
                            f"å˜ä½“ '{variant_name}' æ­¥éª¤ '{step_id}' å¼•ç”¨äº† agent '{pipeline_step.agent}' ä¸­ä¸å­˜åœ¨çš„ flow: {override.flow}\n"
                            f"  å¯ç”¨çš„ flows: {available_flows}\n"
                            f"  ä¿®å¤å»ºè®®: è¯·æ£€æŸ¥ flow åç§°çš„æ‹¼å†™ï¼Œæˆ–åœ¨ agent '{pipeline_step.agent}' ä¸­åˆ›å»ºæ–°çš„ flow"
                        )
        
        # éªŒè¯ testset æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if config.default_testset:
            testset_path = self._resolve_testset_path(config.id, config.default_testset)
            if not testset_path.exists():
                # å°è¯•æŸ¥æ‰¾å¯èƒ½çš„æµ‹è¯•é›†æ–‡ä»¶
                testset_dir = ROOT_DIR / "data" / "pipelines" / config.id / "testsets"
                available_testsets = []
                if testset_dir.exists():
                    available_testsets = [f.name for f in testset_dir.glob("*.jsonl")]
                
                available_list = ", ".join(available_testsets) if available_testsets else "æ— "
                errors.append(
                    f"é»˜è®¤æµ‹è¯•é›†æ–‡ä»¶ä¸å­˜åœ¨: {config.default_testset}\n"
                    f"  æŸ¥æ‰¾è·¯å¾„: {testset_path}\n"
                    f"  å¯ç”¨çš„æµ‹è¯•é›†: {available_list}\n"
                    f"  ä¿®å¤å»ºè®®: è¯·åˆ›å»ºæµ‹è¯•é›†æ–‡ä»¶ï¼Œæˆ–æ›´æ–° default_testset å­—æ®µ"
                )
        
        return errors
    
    def _resolve_testset_path(self, pipeline_id: str, testset_file: str) -> Path:
        """è§£ææµ‹è¯•é›†æ–‡ä»¶è·¯å¾„"""
        # å¦‚æœæ˜¯ç»å¯¹è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨
        if Path(testset_file).is_absolute():
            return Path(testset_file)
        
        # ç›¸å¯¹è·¯å¾„ï¼Œä¼˜å…ˆåœ¨ pipeline ç›®å½•ä¸‹æŸ¥æ‰¾
        pipeline_testset_path = ROOT_DIR / "data" / "pipelines" / pipeline_id / "testsets" / testset_file
        if pipeline_testset_path.exists():
            return pipeline_testset_path
        
        # å…¼å®¹æ—§çš„ data/testsets ç›®å½•
        old_testset_path = ROOT_DIR / "data" / "testsets" / testset_file
        if old_testset_path.exists():
            return old_testset_path
        
        # ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•
        return ROOT_DIR / testset_file


def validate_yaml_schema(data: Dict[str, Any]) -> List[str]:
    """éªŒè¯ YAML æ•°æ®çš„åŸºæœ¬ schema"""
    errors = []
    
    # å¿…éœ€å­—æ®µæ£€æŸ¥
    required_fields = ["id", "name", "steps"]
    for field in required_fields:
        if field not in data:
            errors.append(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
        elif not data[field]:
            errors.append(f"å¿…éœ€å­—æ®µä¸èƒ½ä¸ºç©º: {field}")
    
    # å­—æ®µç±»å‹æ£€æŸ¥
    if "id" in data and not isinstance(data["id"], str):
        errors.append("å­—æ®µ 'id' å¿…é¡»æ˜¯å­—ç¬¦ä¸²")
    
    if "name" in data and not isinstance(data["name"], str):
        errors.append("å­—æ®µ 'name' å¿…é¡»æ˜¯å­—ç¬¦ä¸²")
    
    if "description" in data and not isinstance(data["description"], str):
        errors.append("å­—æ®µ 'description' å¿…é¡»æ˜¯å­—ç¬¦ä¸²")
    
    if "default_testset" in data and not isinstance(data["default_testset"], str):
        errors.append("å­—æ®µ 'default_testset' å¿…é¡»æ˜¯å­—ç¬¦ä¸²")
    
    # éªŒè¯ inputs å­—æ®µ
    if "inputs" in data:
        if not isinstance(data["inputs"], list):
            errors.append("å­—æ®µ 'inputs' å¿…é¡»æ˜¯åˆ—è¡¨")
        else:
            for i, input_item in enumerate(data["inputs"]):
                if isinstance(input_item, dict):
                    if "name" not in input_item:
                        errors.append(f"è¾“å…¥é¡¹ {i} ç¼ºå°‘ 'name' å­—æ®µ")
                elif not isinstance(input_item, str):
                    errors.append(f"è¾“å…¥é¡¹ {i} å¿…é¡»æ˜¯å­—ç¬¦ä¸²æˆ–åŒ…å« 'name' å­—æ®µçš„å­—å…¸")
    
    # éªŒè¯ steps å­—æ®µ
    if "steps" in data:
        if not isinstance(data["steps"], list):
            errors.append("å­—æ®µ 'steps' å¿…é¡»æ˜¯åˆ—è¡¨")
        else:
            step_required_fields = ["id", "agent", "flow", "output_key"]
            for i, step in enumerate(data["steps"]):
                if not isinstance(step, dict):
                    errors.append(f"æ­¥éª¤ {i} å¿…é¡»æ˜¯å­—å…¸")
                    continue
                
                for field in step_required_fields:
                    if field not in step:
                        errors.append(f"æ­¥éª¤ {i} ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
                    elif not step[field]:
                        errors.append(f"æ­¥éª¤ {i} çš„å­—æ®µ '{field}' ä¸èƒ½ä¸ºç©º")
                
                if "input_mapping" in step and not isinstance(step["input_mapping"], dict):
                    errors.append(f"æ­¥éª¤ {i} çš„ 'input_mapping' å¿…é¡»æ˜¯å­—å…¸")
    
    # éªŒè¯ outputs å­—æ®µ
    if "outputs" in data:
        if not isinstance(data["outputs"], list):
            errors.append("å­—æ®µ 'outputs' å¿…é¡»æ˜¯åˆ—è¡¨")
        else:
            for i, output_item in enumerate(data["outputs"]):
                if isinstance(output_item, dict):
                    if "key" not in output_item:
                        errors.append(f"è¾“å‡ºé¡¹ {i} ç¼ºå°‘ 'key' å­—æ®µ")
                elif not isinstance(output_item, str):
                    errors.append(f"è¾“å‡ºé¡¹ {i} å¿…é¡»æ˜¯å­—ç¬¦ä¸²æˆ–åŒ…å« 'key' å­—æ®µçš„å­—å…¸")
    
    # éªŒè¯ baseline å­—æ®µ
    if "baseline" in data:
        baseline = data["baseline"]
        if not isinstance(baseline, dict):
            errors.append("å­—æ®µ 'baseline' å¿…é¡»æ˜¯å­—å…¸")
        else:
            if "name" not in baseline:
                errors.append("Baseline ç¼ºå°‘ 'name' å­—æ®µ")
            if "steps" in baseline and not isinstance(baseline["steps"], dict):
                errors.append("Baseline çš„ 'steps' å­—æ®µå¿…é¡»æ˜¯å­—å…¸")
    
    # éªŒè¯ variants å­—æ®µ
    if "variants" in data:
        if not isinstance(data["variants"], dict):
            errors.append("å­—æ®µ 'variants' å¿…é¡»æ˜¯å­—å…¸")
        else:
            for variant_name, variant in data["variants"].items():
                if not isinstance(variant, dict):
                    errors.append(f"å˜ä½“ '{variant_name}' å¿…é¡»æ˜¯å­—å…¸")
                elif "overrides" in variant and not isinstance(variant["overrides"], dict):
                    errors.append(f"å˜ä½“ '{variant_name}' çš„ 'overrides' å­—æ®µå¿…é¡»æ˜¯å­—å…¸")
    
    return errors


def validate_output_parser_config(parser_config: Dict[str, Any], location: str) -> List[str]:
    """
    éªŒè¯ output_parser é…ç½®çš„æœ‰æ•ˆæ€§
    
    Args:
        parser_config: output_parser é…ç½®å­—å…¸
        location: é…ç½®ä½ç½®æè¿°ï¼ˆç”¨äºé”™è¯¯ä¿¡æ¯ï¼‰
    
    Returns:
        é”™è¯¯åˆ—è¡¨
    """
    errors = []
    
    # éªŒè¯ type å­—æ®µ
    if "type" not in parser_config:
        errors.append(f"{location}: output_parser ç¼ºå°‘ 'type' å­—æ®µ")
        return errors
    
    parser_type = parser_config["type"]
    valid_types = ["json", "pydantic", "list", "none"]
    if parser_type not in valid_types:
        errors.append(
            f"{location}: ä¸æ”¯æŒçš„ output_parser ç±»å‹ '{parser_type}'ã€‚"
            f"æ”¯æŒçš„ç±»å‹: {', '.join(valid_types)}"
        )
    
    # éªŒè¯ JSON parser é…ç½®
    if parser_type == "json" and "schema" in parser_config:
        schema = parser_config["schema"]
        if not isinstance(schema, dict):
            errors.append(f"{location}: JSON schema å¿…é¡»æ˜¯å­—å…¸ç±»å‹")
        else:
            # éªŒè¯ JSON schema çš„åŸºæœ¬ç»“æ„
            schema_errors = _validate_json_schema(schema, location)
            errors.extend(schema_errors)
    
    # éªŒè¯ Pydantic parser é…ç½®
    if parser_type == "pydantic":
        if "pydantic_model" not in parser_config:
            errors.append(f"{location}: Pydantic parser å¿…é¡»æŒ‡å®š 'pydantic_model' å­—æ®µ")
        elif not isinstance(parser_config["pydantic_model"], str):
            errors.append(f"{location}: 'pydantic_model' å¿…é¡»æ˜¯å­—ç¬¦ä¸²")
    
    # éªŒè¯é‡è¯•é…ç½®
    if "max_retries" in parser_config:
        max_retries = parser_config["max_retries"]
        if not isinstance(max_retries, int):
            errors.append(f"{location}: 'max_retries' å¿…é¡»æ˜¯æ•´æ•°")
        elif max_retries < 0:
            errors.append(f"{location}: 'max_retries' å¿…é¡»æ˜¯éè´Ÿæ•´æ•°")
    
    if "retry_on_error" in parser_config:
        if not isinstance(parser_config["retry_on_error"], bool):
            errors.append(f"{location}: 'retry_on_error' å¿…é¡»æ˜¯å¸ƒå°”å€¼")
    
    return errors


def _validate_json_schema(schema: Dict[str, Any], location: str) -> List[str]:
    """
    éªŒè¯ JSON schema çš„åŸºæœ¬æ ¼å¼
    
    Args:
        schema: JSON schema å­—å…¸
        location: é…ç½®ä½ç½®æè¿°
    
    Returns:
        é”™è¯¯åˆ—è¡¨
    """
    errors = []
    
    # éªŒè¯ type å­—æ®µ
    if "type" in schema:
        schema_type = schema["type"]
        valid_schema_types = ["object", "array", "string", "number", "integer", "boolean", "null"]
        if schema_type not in valid_schema_types:
            errors.append(
                f"{location}: JSON schema çš„ type '{schema_type}' æ— æ•ˆã€‚"
                f"æœ‰æ•ˆç±»å‹: {', '.join(valid_schema_types)}"
            )
    
    # éªŒè¯ properties å­—æ®µï¼ˆå¯¹äº object ç±»å‹ï¼‰
    if schema.get("type") == "object" and "properties" in schema:
        properties = schema["properties"]
        if not isinstance(properties, dict):
            errors.append(f"{location}: JSON schema çš„ 'properties' å¿…é¡»æ˜¯å­—å…¸")
    
    # éªŒè¯ required å­—æ®µ
    if "required" in schema:
        required = schema["required"]
        if not isinstance(required, list):
            errors.append(f"{location}: JSON schema çš„ 'required' å¿…é¡»æ˜¯åˆ—è¡¨")
        elif not all(isinstance(item, str) for item in required):
            errors.append(f"{location}: JSON schema çš„ 'required' åˆ—è¡¨ä¸­çš„æ‰€æœ‰é¡¹å¿…é¡»æ˜¯å­—ç¬¦ä¸²")
    
    # éªŒè¯ items å­—æ®µï¼ˆå¯¹äº array ç±»å‹ï¼‰
    if schema.get("type") == "array" and "items" in schema:
        items = schema["items"]
        if not isinstance(items, dict):
            errors.append(f"{location}: JSON schema çš„ 'items' å¿…é¡»æ˜¯å­—å…¸")
    
    return errors


def load_pipeline_config(pipeline_id: str) -> PipelineConfig:
    """åŠ è½½æŒ‡å®š pipeline çš„é…ç½®"""
    # æŸ¥æ‰¾é…ç½®æ–‡ä»¶
    config_path = find_pipeline_config_file(pipeline_id)
    
    try:
        with open(config_path, "r", encoding="utf-8") as f:
            data = yaml.safe_load(f)
    except yaml.YAMLError as e:
        raise create_config_error(
            message=f"YAML è§£æé”™è¯¯: {e}",
            suggestion="è¯·æ£€æŸ¥ YAML æ–‡ä»¶çš„è¯­æ³•ï¼Œç¡®ä¿ç¼©è¿›å’Œæ ¼å¼æ­£ç¡®",
            file_path=str(config_path)
        )
    except FileNotFoundError:
        available_pipelines = list_available_pipelines()
        suggestion = f"å¯ç”¨çš„ pipelines: {', '.join(available_pipelines)}" if available_pipelines else "è¯·å…ˆåˆ›å»º pipeline é…ç½®æ–‡ä»¶"
        raise create_config_error(
            message=f"Pipeline é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}",
            suggestion=suggestion,
            file_path=str(config_path)
        )
    except Exception as e:
        raise create_config_error(
            message=f"è¯»å–é…ç½®æ–‡ä»¶æ—¶å‡ºé”™: {e}",
            suggestion="è¯·æ£€æŸ¥æ–‡ä»¶æƒé™å’Œç£ç›˜ç©ºé—´",
            file_path=str(config_path)
        )
    
    if not isinstance(data, dict):
        raise create_config_error(
            message="é…ç½®æ–‡ä»¶æ ¹èŠ‚ç‚¹å¿…é¡»æ˜¯å­—å…¸",
            suggestion="è¯·ç¡®ä¿ YAML æ–‡ä»¶çš„æ ¹çº§åˆ«æ˜¯å­—å…¸æ ¼å¼",
            file_path=str(config_path)
        )
    
    # Schema éªŒè¯
    schema_errors = validate_yaml_schema(data)
    
    # éªŒè¯ output_parser é…ç½®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if "steps" in data and isinstance(data["steps"], list):
        for i, step in enumerate(data["steps"]):
            if isinstance(step, dict) and "output_parser" in step:
                parser_errors = validate_output_parser_config(
                    step["output_parser"],
                    f"æ­¥éª¤ {i} (id: {step.get('id', 'unknown')})"
                )
                schema_errors.extend(parser_errors)
    
    if schema_errors:
        error_msg = "é…ç½®æ–‡ä»¶ schema éªŒè¯å¤±è´¥:\n" + "\n".join(f"- {error}" for error in schema_errors)
        raise create_config_error(
            message=error_msg,
            suggestion="è¯·æ£€æŸ¥é…ç½®æ–‡ä»¶çš„å­—æ®µåç§°ã€ç±»å‹å’Œå¿…éœ€å­—æ®µ",
            file_path=str(config_path)
        )
    
    # åˆ›å»ºé…ç½®å¯¹è±¡
    try:
        config = PipelineConfig.from_dict(data)
    except Exception as e:
        raise create_config_error(
            message=f"åˆ›å»ºé…ç½®å¯¹è±¡æ—¶å‡ºé”™: {e}",
            suggestion="è¯·æ£€æŸ¥é…ç½®æ–‡ä»¶çš„æ•°æ®æ ¼å¼å’Œå­—æ®µå€¼",
            file_path=str(config_path)
        )
    
    # æ•°æ®éªŒè¯
    validation_errors = config.validate()
    if validation_errors:
        error_msg = "é…ç½®éªŒè¯å¤±è´¥:\n" + "\n".join(f"- {error}" for error in validation_errors)
        raise create_config_error(
            message=error_msg,
            suggestion="è¯·æ£€æŸ¥é…ç½®çš„é€»è¾‘ä¸€è‡´æ€§å’Œæ•°æ®å®Œæ•´æ€§",
            file_path=str(config_path)
        )
    
    # å¼•ç”¨å®Œæ•´æ€§éªŒè¯
    validator = PipelineValidator()
    reference_errors = validator.validate_references(config)
    if reference_errors:
        error_msg = "å¼•ç”¨éªŒè¯å¤±è´¥:\n" + "\n".join(f"- {error}" for error in reference_errors)
        raise create_config_error(
            message=error_msg,
            suggestion="è¯·ç¡®ä¿å¼•ç”¨çš„ agentsã€flows å’Œæ–‡ä»¶éƒ½å­˜åœ¨",
            file_path=str(config_path)
        )
    
    return config


def find_pipeline_config_file(pipeline_id: str) -> Path:
    """æŸ¥æ‰¾ pipeline é…ç½®æ–‡ä»¶"""
    # ä¼˜å…ˆæŸ¥æ‰¾ pipelines/{pipeline_id}.yaml
    config_path = PIPELINES_DIR / f"{pipeline_id}.yaml"
    if config_path.exists():
        return config_path
    
    # æŸ¥æ‰¾ pipelines/{pipeline_id}/pipeline.yaml
    dir_config_path = PIPELINES_DIR / pipeline_id / "pipeline.yaml"
    if dir_config_path.exists():
        return dir_config_path
    
    available_pipelines = list_available_pipelines()
    suggestion = f"å¯ç”¨çš„ pipelines: {', '.join(available_pipelines)}" if available_pipelines else "è¯·å…ˆåˆ›å»º pipeline é…ç½®æ–‡ä»¶"
    raise create_config_error(
        message=f"æ‰¾ä¸åˆ° pipeline '{pipeline_id}' çš„é…ç½®æ–‡ä»¶",
        suggestion=suggestion
    )


def list_available_pipelines() -> List[str]:
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ pipeline ID"""
    if not PIPELINES_DIR.exists():
        return []
    
    pipeline_ids = []
    
    # æŸ¥æ‰¾ pipelines/{pipeline_id}.yaml æ–‡ä»¶
    for yaml_file in PIPELINES_DIR.glob("*.yaml"):
        pipeline_ids.append(yaml_file.stem)
    
    # æŸ¥æ‰¾ pipelines/{pipeline_id}/pipeline.yaml æ–‡ä»¶
    for pipeline_dir in PIPELINES_DIR.iterdir():
        if (pipeline_dir.is_dir() and 
            (pipeline_dir / "pipeline.yaml").exists() and 
            pipeline_dir.name not in pipeline_ids):
            pipeline_ids.append(pipeline_dir.name)
    
    return sorted(pipeline_ids)


def save_pipeline_config(config: PipelineConfig, file_path: Optional[Path] = None) -> Path:
    """ä¿å­˜ pipeline é…ç½®åˆ°æ–‡ä»¶"""
    if file_path is None:
        # ç¡®ä¿ pipelines ç›®å½•å­˜åœ¨
        PIPELINES_DIR.mkdir(parents=True, exist_ok=True)
        file_path = PIPELINES_DIR / f"{config.id}.yaml"
    
    # éªŒè¯é…ç½®
    validation_errors = config.validate()
    if validation_errors:
        raise PipelineConfigError(f"é…ç½®éªŒè¯å¤±è´¥:\n" + "\n".join(f"- {error}" for error in validation_errors))
    
    # è½¬æ¢ä¸ºå­—å…¸å¹¶ä¿å­˜
    data = config.to_dict()
    
    try:
        with open(file_path, "w", encoding="utf-8") as f:
            yaml.dump(data, f, default_flow_style=False, allow_unicode=True, indent=2)
    except Exception as e:
        raise PipelineConfigError(f"ä¿å­˜é…ç½®æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    return file_path


def validate_pipeline_config_file(file_path: Path) -> List[str]:
    """
    éªŒè¯ pipeline é…ç½®æ–‡ä»¶ï¼Œè¿”å›é”™è¯¯åˆ—è¡¨
    
    Args:
        file_path: é…ç½®æ–‡ä»¶è·¯å¾„
    
    Returns:
        é”™è¯¯åˆ—è¡¨ï¼Œæ¯ä¸ªé”™è¯¯åŒ…å«è¯¦ç»†çš„ä½ç½®å’Œä¿®å¤å»ºè®®
    """
    errors = []
    
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            data = yaml.safe_load(f)
    except yaml.YAMLError as e:
        return [
            f"YAML è§£æé”™è¯¯: {e}\n"
            f"  æ–‡ä»¶: {file_path}\n"
            f"  ä¿®å¤å»ºè®®: è¯·æ£€æŸ¥ YAML æ–‡ä»¶çš„è¯­æ³•ï¼Œç¡®ä¿ç¼©è¿›å’Œæ ¼å¼æ­£ç¡®"
        ]
    except FileNotFoundError:
        return [
            f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}\n"
            f"  ä¿®å¤å»ºè®®: è¯·ç¡®è®¤æ–‡ä»¶è·¯å¾„æ­£ç¡®ï¼Œæˆ–åˆ›å»ºæ–°çš„é…ç½®æ–‡ä»¶"
        ]
    except Exception as e:
        return [
            f"è¯»å–é…ç½®æ–‡ä»¶æ—¶å‡ºé”™: {e}\n"
            f"  æ–‡ä»¶: {file_path}\n"
            f"  ä¿®å¤å»ºè®®: è¯·æ£€æŸ¥æ–‡ä»¶æƒé™å’Œç£ç›˜ç©ºé—´"
        ]
    
    if not isinstance(data, dict):
        return [
            "é…ç½®æ–‡ä»¶æ ¹èŠ‚ç‚¹å¿…é¡»æ˜¯å­—å…¸\n"
            f"  æ–‡ä»¶: {file_path}\n"
            f"  ä¿®å¤å»ºè®®: è¯·ç¡®ä¿ YAML æ–‡ä»¶çš„æ ¹çº§åˆ«æ˜¯å­—å…¸æ ¼å¼ï¼ˆé”®å€¼å¯¹ï¼‰"
        ]
    
    # Schema éªŒè¯
    errors.extend(validate_yaml_schema(data))
    
    # éªŒè¯ output_parser é…ç½®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if "steps" in data and isinstance(data["steps"], list):
        for i, step in enumerate(data["steps"]):
            if isinstance(step, dict) and "output_parser" in step:
                parser_errors = validate_output_parser_config(
                    step["output_parser"],
                    f"æ­¥éª¤ {i} (id: {step.get('id', 'unknown')})"
                )
                errors.extend(parser_errors)
    
    # å¦‚æœ schema éªŒè¯å¤±è´¥ï¼Œä¸ç»§ç»­åç»­éªŒè¯
    if errors:
        return errors
    
    try:
        # åˆ›å»ºé…ç½®å¯¹è±¡å¹¶éªŒè¯
        config = PipelineConfig.from_dict(data)
        errors.extend(config.validate())
        
        # å¼•ç”¨å®Œæ•´æ€§éªŒè¯
        validator = PipelineValidator()
        errors.extend(validator.validate_references(config))
        
    except Exception as e:
        errors.append(
            f"é…ç½®å¯¹è±¡åˆ›å»ºæˆ–éªŒè¯æ—¶å‡ºé”™: {e}\n"
            f"  æ–‡ä»¶: {file_path}\n"
            f"  ä¿®å¤å»ºè®®: è¯·æ£€æŸ¥é…ç½®æ–‡ä»¶çš„æ•°æ®æ ¼å¼å’Œå­—æ®µå€¼"
        )
    
    return errors


def format_validation_errors(errors: List[str], file_path: Optional[Path] = None) -> str:
    """
    æ ¼å¼åŒ–éªŒè¯é”™è¯¯ä¸ºæ˜“è¯»çš„å­—ç¬¦ä¸²
    
    Args:
        errors: é”™è¯¯åˆ—è¡¨
        file_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        æ ¼å¼åŒ–çš„é”™è¯¯æ¶ˆæ¯
    """
    if not errors:
        return "âœ… é…ç½®éªŒè¯é€šè¿‡"
    
    lines = []
    lines.append(f"âŒ é…ç½®éªŒè¯å¤±è´¥ï¼Œå‘ç° {len(errors)} ä¸ªé”™è¯¯:")
    
    if file_path:
        lines.append(f"ğŸ“„ æ–‡ä»¶: {file_path}")
    
    lines.append("")
    
    for i, error in enumerate(errors, 1):
        lines.append(f"{i}. {error}")
        lines.append("")
    
    return "\n".join(lines)


def get_pipeline_summary(pipeline_id: str) -> str:
    """è·å– pipeline çš„ç®€è¦ä¿¡æ¯ï¼Œç”¨äºå‘½ä»¤è¡Œå¸®åŠ©"""
    try:
        config = load_pipeline_config(pipeline_id)
        step_count = len(config.steps)
        variant_count = len(config.variants)
        return f"{config.name} ({step_count} æ­¥éª¤, {variant_count} å˜ä½“)"
    except Exception:
        return f"{pipeline_id} (é…ç½®åŠ è½½å¤±è´¥)"