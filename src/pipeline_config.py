# src/pipeline_config.py
"""
Pipeline YAML é…ç½®è§£æå™¨

å¤„ç† pipeline YAML æ–‡ä»¶çš„åŠ è½½ã€éªŒè¯å’Œç®¡ç†ï¼Œ
åŒ…æ‹¬ schema éªŒè¯ã€å¼•ç”¨å®Œæ•´æ€§æ£€æŸ¥å’Œå¾ªç¯ä¾èµ–æ£€æµ‹ã€‚
"""

from __future__ import annotations

import yaml
from pathlib import Path
from typing import Dict, List, Any, Optional, Set
import logging

from .models import PipelineConfig
from .agent_registry import load_agent, list_available_agents, AgentConfig
from .error_handler import create_config_error, create_data_error, handle_error

logger = logging.getLogger(__name__)

# è·å–é¡¹ç›®æ ¹ç›®å½•
ROOT_DIR = Path(__file__).resolve().parent.parent

# Pipeline ç›®å½•åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
PIPELINE_DIRS = [
    ROOT_DIR / "pipelines",           # ç”Ÿäº§ Pipelineï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
    ROOT_DIR / "examples" / "pipelines",  # ç¤ºä¾‹ Pipeline
]

# å‘åå…¼å®¹
PIPELINES_DIR = PIPELINE_DIRS[0]


class PipelineConfigError(Exception):
    """Pipeline é…ç½®é”™è¯¯ï¼ˆå‘åå…¼å®¹ï¼‰"""
    pass


class PipelineValidator:
    """Pipeline é…ç½®éªŒè¯å™¨"""
    
    def __init__(self):
        self.available_agents: Set[str] = set()
        self.agent_flows: Dict[str, Set[str]] = {}
        self._load_agent_info()
    
    def detect_circular_dependencies(self, config: PipelineConfig) -> List[str]:
        """
        æ£€æµ‹ Pipeline æ­¥éª¤é—´çš„å¾ªç¯ä¾èµ–
        
        Args:
            config: Pipeline é…ç½®
        
        Returns:
            é”™è¯¯åˆ—è¡¨ï¼ŒåŒ…å«å¾ªç¯ä¾èµ–çš„è¯¦ç»†ä¿¡æ¯
        """
        errors = []
        
        # æ„å»ºä¾èµ–å›¾
        dependencies: Dict[str, Set[str]] = {}
        step_outputs: Dict[str, str] = {}
        
        for step in config.steps:
            dependencies[step.id] = set()
            step_outputs[step.output_key] = step.id
        
        # åˆ†æè¾“å…¥æ˜ å°„ä¸­çš„ä¾èµ–å…³ç³»
        for step in config.steps:
            for param, source in step.input_mapping.items():
                # å¦‚æœæºæ˜¯å…¶ä»–æ­¥éª¤çš„è¾“å‡º
                if source in step_outputs:
                    source_step = step_outputs[source]
                    if source_step != step.id:  # ä¸èƒ½ä¾èµ–è‡ªå·±
                        dependencies[step.id].add(source_step)
                    else:
                        errors.append(
                            f"æ­¥éª¤ '{step.id}' ä¸èƒ½ä¾èµ–è‡ªå·±çš„è¾“å‡º\n"
                            f"  é—®é¢˜: input_mapping['{param}'] = '{source}'\n"
                            f"  ä¿®å¤å»ºè®®: è¯·ç§»é™¤æ­¤è‡ªå¼•ç”¨ï¼Œæˆ–ä½¿ç”¨å…¶ä»–æ­¥éª¤çš„è¾“å‡º"
                        )
        
        # æ£€æµ‹å¾ªç¯ä¾èµ–
        def find_cycle_path(node: str, visited: Set[str], rec_stack: List[str]) -> Optional[List[str]]:
            """æŸ¥æ‰¾å¾ªç¯ä¾èµ–è·¯å¾„"""
            visited.add(node)
            rec_stack.append(node)
            
            for neighbor in dependencies.get(node, set()):
                if neighbor not in visited:
                    cycle = find_cycle_path(neighbor, visited, rec_stack)
                    if cycle:
                        return cycle
                elif neighbor in rec_stack:
                    # æ‰¾åˆ°å¾ªç¯ï¼Œè¿”å›å¾ªç¯è·¯å¾„
                    cycle_start = rec_stack.index(neighbor)
                    return rec_stack[cycle_start:] + [neighbor]
            
            rec_stack.pop()
            return None
        
        visited: Set[str] = set()
        for step_id in dependencies:
            if step_id not in visited:
                cycle_path = find_cycle_path(step_id, visited, [])
                if cycle_path:
                    cycle_str = " -> ".join(cycle_path)
                    errors.append(
                        f"æ£€æµ‹åˆ°å¾ªç¯ä¾èµ–:\n"
                        f"  å¾ªç¯è·¯å¾„: {cycle_str}\n"
                        f"  ä¿®å¤å»ºè®®: è¯·é‡æ–°è®¾è®¡æ­¥éª¤é—´çš„æ•°æ®æµï¼Œæ‰“ç ´å¾ªç¯ä¾èµ–"
                    )
                    break  # åªæŠ¥å‘Šç¬¬ä¸€ä¸ªå¾ªç¯
        
        return errors
    
    def _load_agent_info(self):
        """åŠ è½½å¯ç”¨çš„ agent å’Œ flow ä¿¡æ¯"""
        try:
            self.available_agents = set(list_available_agents())
            
            for agent_id in self.available_agents:
                try:
                    agent = load_agent(agent_id)
                    self.agent_flows[agent_id] = {flow.name for flow in agent.flows}
                except Exception as e:
                    logger.warning(f"æ— æ³•åŠ è½½ agent {agent_id} çš„é…ç½®: {e}")
                    self.agent_flows[agent_id] = set()
                    
        except Exception as e:
            logger.warning(f"åŠ è½½ agent ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            self.available_agents = set()
            self.agent_flows = {}
    
    def validate_references(self, config: PipelineConfig) -> List[str]:
        """éªŒè¯é…ç½®ä¸­çš„å¼•ç”¨å®Œæ•´æ€§ï¼Œæä¾›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œä¿®å¤å»ºè®®"""
        errors = []
        
        # é¦–å…ˆæ£€æµ‹å¾ªç¯ä¾èµ–
        cycle_errors = self.detect_circular_dependencies(config)
        errors.extend(cycle_errors)
        
        # éªŒè¯æ­¥éª¤ä¸­çš„ agent å’Œ flow å¼•ç”¨
        for step in config.steps:
            # åªéªŒè¯ agent_flow ç±»å‹çš„æ­¥éª¤
            if step.type == "agent_flow":
                # æ£€æŸ¥ agent æ˜¯å¦å­˜åœ¨
                if step.agent not in self.available_agents:
                    available_list = ", ".join(sorted(self.available_agents)) if self.available_agents else "æ— "
                    errors.append(
                        f"æ­¥éª¤ '{step.id}' å¼•ç”¨äº†ä¸å­˜åœ¨çš„ agent: {step.agent}\n"
                        f"  å¯ç”¨çš„ agents: {available_list}\n"
                        f"  ä¿®å¤å»ºè®®: è¯·æ£€æŸ¥ agent ID çš„æ‹¼å†™ï¼Œæˆ–åˆ›å»ºæ–°çš„ agent"
                    )
                    continue
                    
                # æ£€æŸ¥ flow æ˜¯å¦å­˜åœ¨
                agent_flows = self.agent_flows.get(step.agent, set())
                if step.flow not in agent_flows:
                    available_flows = ", ".join(sorted(agent_flows)) if agent_flows else "æ— "
                    errors.append(
                        f"æ­¥éª¤ '{step.id}' å¼•ç”¨äº† agent '{step.agent}' ä¸­ä¸å­˜åœ¨çš„ flow: {step.flow}\n"
                        f"  å¯ç”¨çš„ flows: {available_flows}\n"
                        f"  ä¿®å¤å»ºè®®: è¯·æ£€æŸ¥ flow åç§°çš„æ‹¼å†™ï¼Œæˆ–åœ¨ agent '{step.agent}' ä¸­åˆ›å»ºæ–°çš„ flow"
                    )
            
            elif step.type == "code_node":
                # éªŒè¯ä»£ç èŠ‚ç‚¹çš„æ–‡ä»¶å¼•ç”¨ï¼ˆå¦‚æœä½¿ç”¨å¤–éƒ¨æ–‡ä»¶ï¼‰
                code_file = None
                if step.code_config and step.code_config.code_file:
                    code_file = step.code_config.code_file
                elif step.code_file:
                    code_file = step.code_file
                
                if code_file:
                    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                    code_file_path = Path(code_file)
                    if not code_file_path.is_absolute():
                        code_file_path = ROOT_DIR / code_file
                    
                    if not code_file_path.exists():
                        errors.append(
                            f"æ­¥éª¤ '{step.id}' å¼•ç”¨çš„ä»£ç æ–‡ä»¶ä¸å­˜åœ¨: {code_file}\n"
                            f"  æŸ¥æ‰¾è·¯å¾„: {code_file_path}\n"
                            f"  ä¿®å¤å»ºè®®: è¯·åˆ›å»ºä»£ç æ–‡ä»¶ï¼Œæˆ–æ›´æ–° code_file å­—æ®µ"
                        )
        
        # éªŒè¯ baseline ä¸­çš„ flow å¼•ç”¨
        if config.baseline:
            for step_id, baseline_step in config.baseline.steps.items():
                # æ‰¾åˆ°å¯¹åº”çš„ pipeline æ­¥éª¤
                pipeline_step = None
                for step in config.steps:
                    if step.id == step_id:
                        pipeline_step = step
                        break
                
                if not pipeline_step:
                    step_ids = ", ".join([s.id for s in config.steps])
                    errors.append(
                        f"Baseline å¼•ç”¨äº†ä¸å­˜åœ¨çš„æ­¥éª¤: {step_id}\n"
                        f"  å¯ç”¨çš„æ­¥éª¤ IDs: {step_ids}\n"
                        f"  ä¿®å¤å»ºè®®: è¯·æ£€æŸ¥æ­¥éª¤ ID çš„æ‹¼å†™ï¼Œæˆ–ä» baseline ä¸­ç§»é™¤æ­¤æ­¥éª¤"
                    )
                elif pipeline_step:
                    agent_flows = self.agent_flows.get(pipeline_step.agent, set())
                    if baseline_step.flow not in agent_flows:
                        available_flows = ", ".join(sorted(agent_flows)) if agent_flows else "æ— "
                        errors.append(
                            f"Baseline æ­¥éª¤ '{step_id}' å¼•ç”¨äº† agent '{pipeline_step.agent}' ä¸­ä¸å­˜åœ¨çš„ flow: {baseline_step.flow}\n"
                            f"  å¯ç”¨çš„ flows: {available_flows}\n"
                            f"  ä¿®å¤å»ºè®®: è¯·æ£€æŸ¥ flow åç§°çš„æ‹¼å†™ï¼Œæˆ–åœ¨ agent '{pipeline_step.agent}' ä¸­åˆ›å»ºæ–°çš„ flow"
                        )
        
        # éªŒè¯å˜ä½“ä¸­çš„ flow å¼•ç”¨
        for variant_name, variant in config.variants.items():
            for step_id, override in variant.overrides.items():
                # æ‰¾åˆ°å¯¹åº”çš„ pipeline æ­¥éª¤
                pipeline_step = None
                for step in config.steps:
                    if step.id == step_id:
                        pipeline_step = step
                        break
                
                if not pipeline_step:
                    step_ids = ", ".join([s.id for s in config.steps])
                    errors.append(
                        f"å˜ä½“ '{variant_name}' å¼•ç”¨äº†ä¸å­˜åœ¨çš„æ­¥éª¤: {step_id}\n"
                        f"  å¯ç”¨çš„æ­¥éª¤ IDs: {step_ids}\n"
                        f"  ä¿®å¤å»ºè®®: è¯·æ£€æŸ¥æ­¥éª¤ ID çš„æ‹¼å†™ï¼Œæˆ–ä»å˜ä½“ä¸­ç§»é™¤æ­¤è¦†ç›–"
                    )
                elif override.flow:
                    agent_flows = self.agent_flows.get(pipeline_step.agent, set())
                    if override.flow not in agent_flows:
                        available_flows = ", ".join(sorted(agent_flows)) if agent_flows else "æ— "
                        errors.append(
                            f"å˜ä½“ '{variant_name}' æ­¥éª¤ '{step_id}' å¼•ç”¨äº† agent '{pipeline_step.agent}' ä¸­ä¸å­˜åœ¨çš„ flow: {override.flow}\n"
                            f"  å¯ç”¨çš„ flows: {available_flows}\n"
                            f"  ä¿®å¤å»ºè®®: è¯·æ£€æŸ¥ flow åç§°çš„æ‹¼å†™ï¼Œæˆ–åœ¨ agent '{pipeline_step.agent}' ä¸­åˆ›å»ºæ–°çš„ flow"
                        )
        
        # éªŒè¯ testset æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if config.default_testset:
            testset_path = self._resolve_testset_path(config.id, config.default_testset)
            if not testset_path.exists():
                # å°è¯•æŸ¥æ‰¾å¯èƒ½çš„æµ‹è¯•é›†æ–‡ä»¶
                testset_dir = ROOT_DIR / "data" / "pipelines" / config.id / "testsets"
                available_testsets = []
                if testset_dir.exists():
                    available_testsets = [f.name for f in testset_dir.glob("*.jsonl")]
                
                available_list = ", ".join(available_testsets) if available_testsets else "æ— "
                errors.append(
                    f"é»˜è®¤æµ‹è¯•é›†æ–‡ä»¶ä¸å­˜åœ¨: {config.default_testset}\n"
                    f"  æŸ¥æ‰¾è·¯å¾„: {testset_path}\n"
                    f"  å¯ç”¨çš„æµ‹è¯•é›†: {available_list}\n"
                    f"  ä¿®å¤å»ºè®®: è¯·åˆ›å»ºæµ‹è¯•é›†æ–‡ä»¶ï¼Œæˆ–æ›´æ–° default_testset å­—æ®µ"
                )
        
        return errors
    
    def _resolve_testset_path(self, pipeline_id: str, testset_file: str) -> Path:
        """è§£ææµ‹è¯•é›†æ–‡ä»¶è·¯å¾„"""
        # å¦‚æœæ˜¯ç»å¯¹è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨
        if Path(testset_file).is_absolute():
            return Path(testset_file)
        
        # ç›¸å¯¹è·¯å¾„ï¼Œä¼˜å…ˆåœ¨ pipeline ç›®å½•ä¸‹æŸ¥æ‰¾
        pipeline_testset_path = ROOT_DIR / "data" / "pipelines" / pipeline_id / "testsets" / testset_file
        if pipeline_testset_path.exists():
            return pipeline_testset_path
        
        # å…¼å®¹æ—§çš„ data/testsets ç›®å½•
        old_testset_path = ROOT_DIR / "data" / "testsets" / testset_file
        if old_testset_path.exists():
            return old_testset_path
        
        # ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•
        return ROOT_DIR / testset_file


def validate_code_node_config(step_data: Dict[str, Any], step_index: int) -> List[str]:
    """
    éªŒè¯ä»£ç èŠ‚ç‚¹é…ç½®çš„æœ‰æ•ˆæ€§
    
    Args:
        step_data: æ­¥éª¤é…ç½®å­—å…¸
        step_index: æ­¥éª¤ç´¢å¼•ï¼ˆç”¨äºé”™è¯¯ä¿¡æ¯ï¼‰
    
    Returns:
        é”™è¯¯åˆ—è¡¨
    """
    errors = []
    step_id = step_data.get("id", f"æ­¥éª¤ {step_index}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ code_config
    if "code_config" in step_data:
        code_config = step_data["code_config"]
        if not isinstance(code_config, dict):
            errors.append(f"{step_id}: code_config å¿…é¡»æ˜¯å­—å…¸ç±»å‹")
            return errors
        
        # éªŒè¯ language
        if "language" not in code_config:
            errors.append(f"{step_id}: code_config ç¼ºå°‘ 'language' å­—æ®µ")
        elif code_config["language"] not in ["javascript", "python"]:
            errors.append(
                f"{step_id}: ä¸æ”¯æŒçš„ä»£ç è¯­è¨€ '{code_config['language']}'ã€‚"
                f"æ”¯æŒçš„è¯­è¨€: javascript, python"
            )
        
        # éªŒè¯å¿…é¡»æœ‰ code æˆ– code_file
        has_code = "code" in code_config and code_config["code"]
        has_code_file = "code_file" in code_config and code_config["code_file"]
        
        if not has_code and not has_code_file:
            errors.append(f"{step_id}: code_config å¿…é¡»æŒ‡å®š 'code'ï¼ˆå†…è”ä»£ç ï¼‰æˆ– 'code_file'ï¼ˆå¤–éƒ¨æ–‡ä»¶ï¼‰ä¹‹ä¸€")
        
        if has_code and has_code_file:
            errors.append(f"{step_id}: code_config ä¸èƒ½åŒæ—¶æŒ‡å®š 'code' å’Œ 'code_file'")
        
        # éªŒè¯ timeout
        if "timeout" in code_config:
            timeout = code_config["timeout"]
            if not isinstance(timeout, int):
                errors.append(f"{step_id}: timeout å¿…é¡»æ˜¯æ•´æ•°")
            elif timeout <= 0:
                errors.append(f"{step_id}: timeout å¿…é¡»æ˜¯æ­£æ•´æ•°")
        
        # éªŒè¯ env_vars
        if "env_vars" in code_config:
            env_vars = code_config["env_vars"]
            if not isinstance(env_vars, dict):
                errors.append(f"{step_id}: env_vars å¿…é¡»æ˜¯å­—å…¸ç±»å‹")
    
    else:
        # å‘åå…¼å®¹ï¼šæ£€æŸ¥ç›´æ¥åœ¨ step ä¸­çš„å­—æ®µ
        if "language" not in step_data:
            errors.append(f"{step_id}: ä»£ç èŠ‚ç‚¹å¿…é¡»æŒ‡å®š 'language' å­—æ®µ")
        elif step_data["language"] not in ["javascript", "python"]:
            errors.append(
                f"{step_id}: ä¸æ”¯æŒçš„ä»£ç è¯­è¨€ '{step_data['language']}'ã€‚"
                f"æ”¯æŒçš„è¯­è¨€: javascript, python"
            )
        
        has_code = "code" in step_data and step_data["code"]
        has_code_file = "code_file" in step_data and step_data["code_file"]
        
        if not has_code and not has_code_file:
            errors.append(f"{step_id}: ä»£ç èŠ‚ç‚¹å¿…é¡»æŒ‡å®š 'code'ï¼ˆå†…è”ä»£ç ï¼‰æˆ– 'code_file'ï¼ˆå¤–éƒ¨æ–‡ä»¶ï¼‰ä¹‹ä¸€")
        
        if has_code and has_code_file:
            errors.append(f"{step_id}: ä»£ç èŠ‚ç‚¹ä¸èƒ½åŒæ—¶æŒ‡å®š 'code' å’Œ 'code_file'")
    
    return errors


def validate_yaml_schema(data: Dict[str, Any]) -> List[str]:
    """éªŒè¯ YAML æ•°æ®çš„åŸºæœ¬ schema"""
    errors = []
    
    # å¿…éœ€å­—æ®µæ£€æŸ¥
    required_fields = ["id", "name", "steps"]
    for field in required_fields:
        if field not in data:
            errors.append(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
        elif not data[field]:
            errors.append(f"å¿…éœ€å­—æ®µä¸èƒ½ä¸ºç©º: {field}")
    
    # å­—æ®µç±»å‹æ£€æŸ¥
    if "id" in data and not isinstance(data["id"], str):
        errors.append("å­—æ®µ 'id' å¿…é¡»æ˜¯å­—ç¬¦ä¸²")
    
    if "name" in data and not isinstance(data["name"], str):
        errors.append("å­—æ®µ 'name' å¿…é¡»æ˜¯å­—ç¬¦ä¸²")
    
    if "description" in data and not isinstance(data["description"], str):
        errors.append("å­—æ®µ 'description' å¿…é¡»æ˜¯å­—ç¬¦ä¸²")
    
    if "default_testset" in data and not isinstance(data["default_testset"], str):
        errors.append("å­—æ®µ 'default_testset' å¿…é¡»æ˜¯å­—ç¬¦ä¸²")
    
    # éªŒè¯ inputs å­—æ®µ
    if "inputs" in data:
        if not isinstance(data["inputs"], list):
            errors.append("å­—æ®µ 'inputs' å¿…é¡»æ˜¯åˆ—è¡¨")
        else:
            for i, input_item in enumerate(data["inputs"]):
                if isinstance(input_item, dict):
                    if "name" not in input_item:
                        errors.append(f"è¾“å…¥é¡¹ {i} ç¼ºå°‘ 'name' å­—æ®µ")
                elif not isinstance(input_item, str):
                    errors.append(f"è¾“å…¥é¡¹ {i} å¿…é¡»æ˜¯å­—ç¬¦ä¸²æˆ–åŒ…å« 'name' å­—æ®µçš„å­—å…¸")
    
    # éªŒè¯ steps å­—æ®µ
    if "steps" in data:
        if not isinstance(data["steps"], list):
            errors.append("å­—æ®µ 'steps' å¿…é¡»æ˜¯åˆ—è¡¨")
        else:
            for i, step in enumerate(data["steps"]):
                if not isinstance(step, dict):
                    errors.append(f"æ­¥éª¤ {i} å¿…é¡»æ˜¯å­—å…¸")
                    continue
                
                # éªŒè¯åŸºæœ¬å¿…éœ€å­—æ®µ
                if "id" not in step:
                    errors.append(f"æ­¥éª¤ {i} ç¼ºå°‘å¿…éœ€å­—æ®µ: id")
                elif not step["id"]:
                    errors.append(f"æ­¥éª¤ {i} çš„å­—æ®µ 'id' ä¸èƒ½ä¸ºç©º")
                
                if "output_key" not in step:
                    errors.append(f"æ­¥éª¤ {i} ç¼ºå°‘å¿…éœ€å­—æ®µ: output_key")
                elif not step["output_key"]:
                    errors.append(f"æ­¥éª¤ {i} çš„å­—æ®µ 'output_key' ä¸èƒ½ä¸ºç©º")
                
                # æ ¹æ®æ­¥éª¤ç±»å‹éªŒè¯
                step_type = step.get("type", "agent_flow")
                
                if step_type == "agent_flow":
                    # Agent Flow æ­¥éª¤éœ€è¦ agent å’Œ flow å­—æ®µ
                    if "agent" not in step:
                        errors.append(f"æ­¥éª¤ {i} (agent_flow) ç¼ºå°‘å¿…éœ€å­—æ®µ: agent")
                    elif not step["agent"]:
                        errors.append(f"æ­¥éª¤ {i} çš„å­—æ®µ 'agent' ä¸èƒ½ä¸ºç©º")
                    
                    if "flow" not in step:
                        errors.append(f"æ­¥éª¤ {i} (agent_flow) ç¼ºå°‘å¿…éœ€å­—æ®µ: flow")
                    elif not step["flow"]:
                        errors.append(f"æ­¥éª¤ {i} çš„å­—æ®µ 'flow' ä¸èƒ½ä¸ºç©º")
                
                elif step_type == "code_node":
                    # éªŒè¯ä»£ç èŠ‚ç‚¹é…ç½®
                    code_errors = validate_code_node_config(step, i)
                    errors.extend(code_errors)
                
                elif step_type == "batch_aggregator":
                    # éªŒè¯æ‰¹é‡èšåˆé…ç½®
                    if "aggregation_strategy" not in step:
                        errors.append(f"æ­¥éª¤ {i} (batch_aggregator) ç¼ºå°‘å¿…éœ€å­—æ®µ: aggregation_strategy")
                    elif step["aggregation_strategy"] not in ["concat", "stats", "filter", "group", "summary", "custom"]:
                        errors.append(
                            f"æ­¥éª¤ {i}: ä¸æ”¯æŒçš„èšåˆç­–ç•¥ '{step['aggregation_strategy']}'ã€‚"
                            f"æ”¯æŒçš„ç­–ç•¥: concat, stats, filter, group, summary, custom"
                        )
                    
                    # éªŒè¯ç­–ç•¥ç‰¹å®šå­—æ®µ
                    aggregation_strategy = step.get("aggregation_strategy")
                    
                    if aggregation_strategy == "custom":
                        # Accept either 'code', 'aggregation_code', or 'code_file'
                        has_code = "code" in step or "aggregation_code" in step or "code_file" in step
                        if not has_code:
                            errors.append(f"æ­¥éª¤ {i}: è‡ªå®šä¹‰èšåˆç­–ç•¥å¿…é¡»æŒ‡å®š 'code', 'aggregation_code' æˆ– 'code_file'")
                        if "language" not in step:
                            errors.append(f"æ­¥éª¤ {i}: è‡ªå®šä¹‰èšåˆç­–ç•¥å¿…é¡»æŒ‡å®š 'language' (python æˆ– javascript)")
                    
                    if aggregation_strategy == "stats" and "fields" not in step:
                        errors.append(f"æ­¥éª¤ {i}: stats èšåˆç­–ç•¥å¿…é¡»æŒ‡å®š 'fields' å­—æ®µåˆ—è¡¨")
                    
                    if aggregation_strategy == "filter" and "condition" not in step:
                        errors.append(f"æ­¥éª¤ {i}: filter èšåˆç­–ç•¥å¿…é¡»æŒ‡å®š 'condition' è¿‡æ»¤æ¡ä»¶")
                    
                    if aggregation_strategy == "group" and "group_by" not in step:
                        errors.append(f"æ­¥éª¤ {i}: group èšåˆç­–ç•¥å¿…é¡»æŒ‡å®š 'group_by' åˆ†ç»„å­—æ®µ")
                    
                    if aggregation_strategy == "summary" and "summary_fields" not in step:
                        errors.append(f"æ­¥éª¤ {i}: summary èšåˆç­–ç•¥å¿…é¡»æŒ‡å®š 'summary_fields' æ±‡æ€»å­—æ®µåˆ—è¡¨")
                
                else:
                    errors.append(
                        f"æ­¥éª¤ {i}: ä¸æ”¯æŒçš„æ­¥éª¤ç±»å‹ '{step_type}'ã€‚"
                        f"æ”¯æŒçš„ç±»å‹: agent_flow, code_node, batch_aggregator"
                    )
                
                # éªŒè¯æ‰¹é‡å¤„ç†é…ç½®ï¼ˆé€‚ç”¨äºæ‰€æœ‰æ­¥éª¤ç±»å‹ï¼‰
                if step.get("batch_mode", False):
                    batch_size = step.get("batch_size", 10)
                    max_workers = step.get("max_workers", 4)
                    
                    if not isinstance(batch_size, int):
                        errors.append(f"æ­¥éª¤ {i}: 'batch_size' å¿…é¡»æ˜¯æ•´æ•°")
                    elif batch_size <= 0:
                        errors.append(f"æ­¥éª¤ {i}: 'batch_size' å¿…é¡»æ˜¯æ­£æ•´æ•°")
                    
                    if not isinstance(max_workers, int):
                        errors.append(f"æ­¥éª¤ {i}: 'max_workers' å¿…é¡»æ˜¯æ•´æ•°")
                    elif max_workers <= 0:
                        errors.append(f"æ­¥éª¤ {i}: 'max_workers' å¿…é¡»æ˜¯æ­£æ•´æ•°")
                    
                    if "concurrent" in step and not isinstance(step["concurrent"], bool):
                        errors.append(f"æ­¥éª¤ {i}: 'concurrent' å¿…é¡»æ˜¯å¸ƒå°”å€¼")
                
                # éªŒè¯ input_mapping
                if "input_mapping" in step and not isinstance(step["input_mapping"], dict):
                    errors.append(f"æ­¥éª¤ {i} çš„ 'input_mapping' å¿…é¡»æ˜¯å­—å…¸")
    
    # éªŒè¯ outputs å­—æ®µ
    if "outputs" in data:
        if not isinstance(data["outputs"], list):
            errors.append("å­—æ®µ 'outputs' å¿…é¡»æ˜¯åˆ—è¡¨")
        else:
            for i, output_item in enumerate(data["outputs"]):
                if isinstance(output_item, dict):
                    if "key" not in output_item:
                        errors.append(f"è¾“å‡ºé¡¹ {i} ç¼ºå°‘ 'key' å­—æ®µ")
                elif not isinstance(output_item, str):
                    errors.append(f"è¾“å‡ºé¡¹ {i} å¿…é¡»æ˜¯å­—ç¬¦ä¸²æˆ–åŒ…å« 'key' å­—æ®µçš„å­—å…¸")
    
    # éªŒè¯ baseline å­—æ®µ
    if "baseline" in data:
        baseline = data["baseline"]
        if not isinstance(baseline, dict):
            errors.append("å­—æ®µ 'baseline' å¿…é¡»æ˜¯å­—å…¸")
        else:
            if "name" not in baseline:
                errors.append("Baseline ç¼ºå°‘ 'name' å­—æ®µ")
            if "steps" in baseline and not isinstance(baseline["steps"], dict):
                errors.append("Baseline çš„ 'steps' å­—æ®µå¿…é¡»æ˜¯å­—å…¸")
    
    # éªŒè¯ variants å­—æ®µ
    if "variants" in data:
        if not isinstance(data["variants"], dict):
            errors.append("å­—æ®µ 'variants' å¿…é¡»æ˜¯å­—å…¸")
        else:
            for variant_name, variant in data["variants"].items():
                if not isinstance(variant, dict):
                    errors.append(f"å˜ä½“ '{variant_name}' å¿…é¡»æ˜¯å­—å…¸")
                elif "overrides" in variant and not isinstance(variant["overrides"], dict):
                    errors.append(f"å˜ä½“ '{variant_name}' çš„ 'overrides' å­—æ®µå¿…é¡»æ˜¯å­—å…¸")
    
    return errors


def validate_output_parser_config(parser_config: Dict[str, Any], location: str) -> List[str]:
    """
    éªŒè¯ output_parser é…ç½®çš„æœ‰æ•ˆæ€§
    
    Args:
        parser_config: output_parser é…ç½®å­—å…¸
        location: é…ç½®ä½ç½®æè¿°ï¼ˆç”¨äºé”™è¯¯ä¿¡æ¯ï¼‰
    
    Returns:
        é”™è¯¯åˆ—è¡¨
    """
    errors = []
    
    # éªŒè¯ type å­—æ®µ
    if "type" not in parser_config:
        errors.append(f"{location}: output_parser ç¼ºå°‘ 'type' å­—æ®µ")
        return errors
    
    parser_type = parser_config["type"]
    valid_types = ["json", "pydantic", "list", "none"]
    if parser_type not in valid_types:
        errors.append(
            f"{location}: ä¸æ”¯æŒçš„ output_parser ç±»å‹ '{parser_type}'ã€‚"
            f"æ”¯æŒçš„ç±»å‹: {', '.join(valid_types)}"
        )
    
    # éªŒè¯ JSON parser é…ç½®
    if parser_type == "json" and "schema" in parser_config:
        schema = parser_config["schema"]
        if not isinstance(schema, dict):
            errors.append(f"{location}: JSON schema å¿…é¡»æ˜¯å­—å…¸ç±»å‹")
        else:
            # éªŒè¯ JSON schema çš„åŸºæœ¬ç»“æ„
            schema_errors = _validate_json_schema(schema, location)
            errors.extend(schema_errors)
    
    # éªŒè¯ Pydantic parser é…ç½®
    if parser_type == "pydantic":
        if "pydantic_model" not in parser_config:
            errors.append(f"{location}: Pydantic parser å¿…é¡»æŒ‡å®š 'pydantic_model' å­—æ®µ")
        elif not isinstance(parser_config["pydantic_model"], str):
            errors.append(f"{location}: 'pydantic_model' å¿…é¡»æ˜¯å­—ç¬¦ä¸²")
    
    # éªŒè¯é‡è¯•é…ç½®
    if "max_retries" in parser_config:
        max_retries = parser_config["max_retries"]
        if not isinstance(max_retries, int):
            errors.append(f"{location}: 'max_retries' å¿…é¡»æ˜¯æ•´æ•°")
        elif max_retries < 0:
            errors.append(f"{location}: 'max_retries' å¿…é¡»æ˜¯éè´Ÿæ•´æ•°")
    
    if "retry_on_error" in parser_config:
        if not isinstance(parser_config["retry_on_error"], bool):
            errors.append(f"{location}: 'retry_on_error' å¿…é¡»æ˜¯å¸ƒå°”å€¼")
    
    return errors


def _validate_json_schema(schema: Dict[str, Any], location: str) -> List[str]:
    """
    éªŒè¯ JSON schema çš„åŸºæœ¬æ ¼å¼
    
    Args:
        schema: JSON schema å­—å…¸
        location: é…ç½®ä½ç½®æè¿°
    
    Returns:
        é”™è¯¯åˆ—è¡¨
    """
    errors = []
    
    # éªŒè¯ type å­—æ®µ
    if "type" in schema:
        schema_type = schema["type"]
        valid_schema_types = ["object", "array", "string", "number", "integer", "boolean", "null"]
        if schema_type not in valid_schema_types:
            errors.append(
                f"{location}: JSON schema çš„ type '{schema_type}' æ— æ•ˆã€‚"
                f"æœ‰æ•ˆç±»å‹: {', '.join(valid_schema_types)}"
            )
    
    # éªŒè¯ properties å­—æ®µï¼ˆå¯¹äº object ç±»å‹ï¼‰
    if schema.get("type") == "object" and "properties" in schema:
        properties = schema["properties"]
        if not isinstance(properties, dict):
            errors.append(f"{location}: JSON schema çš„ 'properties' å¿…é¡»æ˜¯å­—å…¸")
    
    # éªŒè¯ required å­—æ®µ
    if "required" in schema:
        required = schema["required"]
        if not isinstance(required, list):
            errors.append(f"{location}: JSON schema çš„ 'required' å¿…é¡»æ˜¯åˆ—è¡¨")
        elif not all(isinstance(item, str) for item in required):
            errors.append(f"{location}: JSON schema çš„ 'required' åˆ—è¡¨ä¸­çš„æ‰€æœ‰é¡¹å¿…é¡»æ˜¯å­—ç¬¦ä¸²")
    
    # éªŒè¯ items å­—æ®µï¼ˆå¯¹äº array ç±»å‹ï¼‰
    if schema.get("type") == "array" and "items" in schema:
        items = schema["items"]
        if not isinstance(items, dict):
            errors.append(f"{location}: JSON schema çš„ 'items' å¿…é¡»æ˜¯å­—å…¸")
    
    return errors


def load_pipeline_config(pipeline_id: str) -> PipelineConfig:
    """åŠ è½½æŒ‡å®š pipeline çš„é…ç½®"""
    # æŸ¥æ‰¾é…ç½®æ–‡ä»¶
    config_path = find_pipeline_config_file(pipeline_id)
    
    try:
        with open(config_path, "r", encoding="utf-8") as f:
            data = yaml.safe_load(f)
    except yaml.YAMLError as e:
        raise create_config_error(
            message=f"YAML è§£æé”™è¯¯: {e}",
            suggestion="è¯·æ£€æŸ¥ YAML æ–‡ä»¶çš„è¯­æ³•ï¼Œç¡®ä¿ç¼©è¿›å’Œæ ¼å¼æ­£ç¡®",
            file_path=str(config_path)
        )
    except FileNotFoundError:
        available_pipelines = list_available_pipelines()
        suggestion = f"å¯ç”¨çš„ pipelines: {', '.join(available_pipelines)}" if available_pipelines else "è¯·å…ˆåˆ›å»º pipeline é…ç½®æ–‡ä»¶"
        raise create_config_error(
            message=f"Pipeline é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}",
            suggestion=suggestion,
            file_path=str(config_path)
        )
    except Exception as e:
        raise create_config_error(
            message=f"è¯»å–é…ç½®æ–‡ä»¶æ—¶å‡ºé”™: {e}",
            suggestion="è¯·æ£€æŸ¥æ–‡ä»¶æƒé™å’Œç£ç›˜ç©ºé—´",
            file_path=str(config_path)
        )
    
    if not isinstance(data, dict):
        raise create_config_error(
            message="é…ç½®æ–‡ä»¶æ ¹èŠ‚ç‚¹å¿…é¡»æ˜¯å­—å…¸",
            suggestion="è¯·ç¡®ä¿ YAML æ–‡ä»¶çš„æ ¹çº§åˆ«æ˜¯å­—å…¸æ ¼å¼",
            file_path=str(config_path)
        )
    
    # Schema éªŒè¯
    schema_errors = validate_yaml_schema(data)
    
    # éªŒè¯ output_parser é…ç½®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if "steps" in data and isinstance(data["steps"], list):
        for i, step in enumerate(data["steps"]):
            if isinstance(step, dict) and "output_parser" in step:
                parser_errors = validate_output_parser_config(
                    step["output_parser"],
                    f"æ­¥éª¤ {i} (id: {step.get('id', 'unknown')})"
                )
                schema_errors.extend(parser_errors)
    
    if schema_errors:
        error_msg = "é…ç½®æ–‡ä»¶ schema éªŒè¯å¤±è´¥:\n" + "\n".join(f"- {error}" for error in schema_errors)
        raise create_config_error(
            message=error_msg,
            suggestion="è¯·æ£€æŸ¥é…ç½®æ–‡ä»¶çš„å­—æ®µåç§°ã€ç±»å‹å’Œå¿…éœ€å­—æ®µ",
            file_path=str(config_path)
        )
    
    # åˆ›å»ºé…ç½®å¯¹è±¡
    try:
        config = PipelineConfig.from_dict(data)
    except Exception as e:
        raise create_config_error(
            message=f"åˆ›å»ºé…ç½®å¯¹è±¡æ—¶å‡ºé”™: {e}",
            suggestion="è¯·æ£€æŸ¥é…ç½®æ–‡ä»¶çš„æ•°æ®æ ¼å¼å’Œå­—æ®µå€¼",
            file_path=str(config_path)
        )
    
    # æ•°æ®éªŒè¯
    validation_errors = config.validate()
    if validation_errors:
        error_msg = "é…ç½®éªŒè¯å¤±è´¥:\n" + "\n".join(f"- {error}" for error in validation_errors)
        raise create_config_error(
            message=error_msg,
            suggestion="è¯·æ£€æŸ¥é…ç½®çš„é€»è¾‘ä¸€è‡´æ€§å’Œæ•°æ®å®Œæ•´æ€§",
            file_path=str(config_path)
        )
    
    # å¼•ç”¨å®Œæ•´æ€§éªŒè¯
    validator = PipelineValidator()
    reference_errors = validator.validate_references(config)
    if reference_errors:
        error_msg = "å¼•ç”¨éªŒè¯å¤±è´¥:\n" + "\n".join(f"- {error}" for error in reference_errors)
        raise create_config_error(
            message=error_msg,
            suggestion="è¯·ç¡®ä¿å¼•ç”¨çš„ agentsã€flows å’Œæ–‡ä»¶éƒ½å­˜åœ¨",
            file_path=str(config_path)
        )
    
    return config


def find_pipeline_config_file(pipeline_id: str) -> Path:
    """æŸ¥æ‰¾ pipeline é…ç½®æ–‡ä»¶ï¼ˆæ”¯æŒå¤šç›®å½•ï¼‰"""
    # åœ¨å¤šä¸ªç›®å½•ä¸­æŸ¥æ‰¾
    for base_dir in PIPELINE_DIRS:
        if not base_dir.exists():
            continue
        
        # æŸ¥æ‰¾ {base_dir}/{pipeline_id}.yaml
        config_path = base_dir / f"{pipeline_id}.yaml"
        if config_path.exists():
            return config_path
        
        # æŸ¥æ‰¾ {base_dir}/{pipeline_id}/pipeline.yaml
        dir_config_path = base_dir / pipeline_id / "pipeline.yaml"
        if dir_config_path.exists():
            return dir_config_path
    
    available_pipelines = list_available_pipelines()
    suggestion = f"å¯ç”¨çš„ pipelines: {', '.join(available_pipelines)}" if available_pipelines else "è¯·å…ˆåˆ›å»º pipeline é…ç½®æ–‡ä»¶"
    raise create_config_error(
        message=f"æ‰¾ä¸åˆ° pipeline '{pipeline_id}' çš„é…ç½®æ–‡ä»¶",
        suggestion=suggestion
    )


def list_available_pipelines() -> List[str]:
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ pipeline IDï¼ˆæ”¯æŒå¤šç›®å½•ï¼‰"""
    pipeline_ids = set()
    
    # éå†æ‰€æœ‰ Pipeline ç›®å½•
    for base_dir in PIPELINE_DIRS:
        if not base_dir.exists():
            continue
        
        # æŸ¥æ‰¾ {base_dir}/{pipeline_id}.yaml æ–‡ä»¶
        for yaml_file in base_dir.glob("*.yaml"):
            pipeline_ids.add(yaml_file.stem)
        
        # æŸ¥æ‰¾ {base_dir}/{pipeline_id}/pipeline.yaml æ–‡ä»¶
        for pipeline_dir in base_dir.iterdir():
            if (pipeline_dir.is_dir() and 
                (pipeline_dir / "pipeline.yaml").exists()):
                pipeline_ids.add(pipeline_dir.name)
    
    return sorted(pipeline_ids)


def save_pipeline_config(config: PipelineConfig, file_path: Optional[Path] = None) -> Path:
    """ä¿å­˜ pipeline é…ç½®åˆ°æ–‡ä»¶"""
    if file_path is None:
        # ç¡®ä¿ pipelines ç›®å½•å­˜åœ¨
        PIPELINES_DIR.mkdir(parents=True, exist_ok=True)
        file_path = PIPELINES_DIR / f"{config.id}.yaml"
    
    # éªŒè¯é…ç½®
    validation_errors = config.validate()
    if validation_errors:
        raise PipelineConfigError(f"é…ç½®éªŒè¯å¤±è´¥:\n" + "\n".join(f"- {error}" for error in validation_errors))
    
    # è½¬æ¢ä¸ºå­—å…¸å¹¶ä¿å­˜
    data = config.to_dict()
    
    try:
        with open(file_path, "w", encoding="utf-8") as f:
            yaml.dump(data, f, default_flow_style=False, allow_unicode=True, indent=2)
    except Exception as e:
        raise PipelineConfigError(f"ä¿å­˜é…ç½®æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    return file_path


def validate_pipeline_config_file(file_path: Path) -> List[str]:
    """
    éªŒè¯ pipeline é…ç½®æ–‡ä»¶ï¼Œè¿”å›é”™è¯¯åˆ—è¡¨
    
    Args:
        file_path: é…ç½®æ–‡ä»¶è·¯å¾„
    
    Returns:
        é”™è¯¯åˆ—è¡¨ï¼Œæ¯ä¸ªé”™è¯¯åŒ…å«è¯¦ç»†çš„ä½ç½®å’Œä¿®å¤å»ºè®®
    """
    errors = []
    
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            data = yaml.safe_load(f)
    except yaml.YAMLError as e:
        return [
            f"YAML è§£æé”™è¯¯: {e}\n"
            f"  æ–‡ä»¶: {file_path}\n"
            f"  ä¿®å¤å»ºè®®: è¯·æ£€æŸ¥ YAML æ–‡ä»¶çš„è¯­æ³•ï¼Œç¡®ä¿ç¼©è¿›å’Œæ ¼å¼æ­£ç¡®"
        ]
    except FileNotFoundError:
        return [
            f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}\n"
            f"  ä¿®å¤å»ºè®®: è¯·ç¡®è®¤æ–‡ä»¶è·¯å¾„æ­£ç¡®ï¼Œæˆ–åˆ›å»ºæ–°çš„é…ç½®æ–‡ä»¶"
        ]
    except Exception as e:
        return [
            f"è¯»å–é…ç½®æ–‡ä»¶æ—¶å‡ºé”™: {e}\n"
            f"  æ–‡ä»¶: {file_path}\n"
            f"  ä¿®å¤å»ºè®®: è¯·æ£€æŸ¥æ–‡ä»¶æƒé™å’Œç£ç›˜ç©ºé—´"
        ]
    
    if not isinstance(data, dict):
        return [
            "é…ç½®æ–‡ä»¶æ ¹èŠ‚ç‚¹å¿…é¡»æ˜¯å­—å…¸\n"
            f"  æ–‡ä»¶: {file_path}\n"
            f"  ä¿®å¤å»ºè®®: è¯·ç¡®ä¿ YAML æ–‡ä»¶çš„æ ¹çº§åˆ«æ˜¯å­—å…¸æ ¼å¼ï¼ˆé”®å€¼å¯¹ï¼‰"
        ]
    
    # Schema éªŒè¯
    errors.extend(validate_yaml_schema(data))
    
    # éªŒè¯ output_parser é…ç½®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if "steps" in data and isinstance(data["steps"], list):
        for i, step in enumerate(data["steps"]):
            if isinstance(step, dict) and "output_parser" in step:
                parser_errors = validate_output_parser_config(
                    step["output_parser"],
                    f"æ­¥éª¤ {i} (id: {step.get('id', 'unknown')})"
                )
                errors.extend(parser_errors)
    
    # å¦‚æœ schema éªŒè¯å¤±è´¥ï¼Œä¸ç»§ç»­åç»­éªŒè¯
    if errors:
        return errors
    
    try:
        # åˆ›å»ºé…ç½®å¯¹è±¡å¹¶éªŒè¯
        config = PipelineConfig.from_dict(data)
        errors.extend(config.validate())
        
        # å¼•ç”¨å®Œæ•´æ€§éªŒè¯
        validator = PipelineValidator()
        errors.extend(validator.validate_references(config))
        
    except Exception as e:
        errors.append(
            f"é…ç½®å¯¹è±¡åˆ›å»ºæˆ–éªŒè¯æ—¶å‡ºé”™: {e}\n"
            f"  æ–‡ä»¶: {file_path}\n"
            f"  ä¿®å¤å»ºè®®: è¯·æ£€æŸ¥é…ç½®æ–‡ä»¶çš„æ•°æ®æ ¼å¼å’Œå­—æ®µå€¼"
        )
    
    return errors


def format_validation_errors(errors: List[str], file_path: Optional[Path] = None) -> str:
    """
    æ ¼å¼åŒ–éªŒè¯é”™è¯¯ä¸ºæ˜“è¯»çš„å­—ç¬¦ä¸²
    
    Args:
        errors: é”™è¯¯åˆ—è¡¨
        file_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        æ ¼å¼åŒ–çš„é”™è¯¯æ¶ˆæ¯
    """
    if not errors:
        return "âœ… é…ç½®éªŒè¯é€šè¿‡"
    
    lines = []
    lines.append(f"âŒ é…ç½®éªŒè¯å¤±è´¥ï¼Œå‘ç° {len(errors)} ä¸ªé”™è¯¯:")
    
    if file_path:
        lines.append(f"ğŸ“„ æ–‡ä»¶: {file_path}")
    
    lines.append("")
    
    for i, error in enumerate(errors, 1):
        lines.append(f"{i}. {error}")
        lines.append("")
    
    return "\n".join(lines)


def get_pipeline_summary(pipeline_id: str) -> str:
    """è·å– pipeline çš„ç®€è¦ä¿¡æ¯ï¼Œç”¨äºå‘½ä»¤è¡Œå¸®åŠ©"""
    try:
        config = load_pipeline_config(pipeline_id)
        step_count = len(config.steps)
        variant_count = len(config.variants)
        return f"{config.name} ({step_count} æ­¥éª¤, {variant_count} å˜ä½“)"
    except Exception:
        return f"{pipeline_id} (é…ç½®åŠ è½½å¤±è´¥)"