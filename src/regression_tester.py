# src/regression_tester.py
"""
å›å½’æµ‹è¯•æ‰§è¡Œå™¨

å¤„ç†å›å½’æµ‹è¯•å·¥ä½œæµï¼Œæ¯”è¾ƒæ–°ç‰ˆæœ¬ä¸ baseline æ€§èƒ½ï¼Œ
æ”¯æŒ pipeline å’Œ agent çº§åˆ«çš„å›å½’æµ‹è¯•ã€‚
"""

from __future__ import annotations

import json
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime

from .baseline_manager import BaselineManager, BaselineSnapshot
from .pipeline_eval import PipelineEvaluator, PipelineComparator
from .models import (
    PipelineConfig, EvaluationResult, ComparisonReport, RegressionCase
)
from .pipeline_config import load_pipeline_config
from .agent_registry import load_agent
from .data_manager import DataManager
from .testset_filter import TestsetFilter

logger = logging.getLogger(__name__)


@dataclass
class RegressionTestConfig:
    """å›å½’æµ‹è¯•é…ç½®"""
    entity_type: str  # "agent" or "pipeline"
    entity_id: str
    baseline_name: str
    variant_name: str
    testset_path: Optional[str] = None
    include_tags: List[str] = field(default_factory=list)
    exclude_tags: List[str] = field(default_factory=list)
    score_threshold: float = 0.5  # åˆ†æ•°ä¸‹é™é˜ˆå€¼
    must_have_check: bool = True  # æ˜¯å¦æ£€æŸ¥ must_have å¤±è´¥
    apply_rules: bool = True
    apply_judge: bool = True
    limit: int = 0  # é™åˆ¶æ ·æœ¬æ•°é‡ï¼Œ0è¡¨ç¤ºå…¨éƒ¨
    
    def validate(self) -> List[str]:
        """éªŒè¯é…ç½®"""
        errors = []
        
        if self.entity_type not in ["agent", "pipeline"]:
            errors.append(f"ä¸æ”¯æŒçš„å®ä½“ç±»å‹: {self.entity_type}")
        
        if not self.entity_id:
            errors.append("å®ä½“ ID ä¸èƒ½ä¸ºç©º")
        
        if not self.baseline_name:
            errors.append("Baseline åç§°ä¸èƒ½ä¸ºç©º")
        
        if not self.variant_name:
            errors.append("å˜ä½“åç§°ä¸èƒ½ä¸ºç©º")
        
        if self.score_threshold < 0:
            errors.append("åˆ†æ•°é˜ˆå€¼ä¸èƒ½ä¸ºè´Ÿæ•°")
        
        return errors


@dataclass
class RegressionTestResult:
    """å›å½’æµ‹è¯•ç»“æœ"""
    config: RegressionTestConfig
    baseline_snapshot: BaselineSnapshot
    variant_results: List[EvaluationResult]
    regression_cases: List[RegressionCase] = field(default_factory=list)
    comparison_report: Optional[ComparisonReport] = None
    summary: str = ""
    created_at: datetime = field(default_factory=datetime.now)
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "config": {
                "entity_type": self.config.entity_type,
                "entity_id": self.config.entity_id,
                "baseline_name": self.config.baseline_name,
                "variant_name": self.config.variant_name,
                "testset_path": self.config.testset_path,
                "include_tags": self.config.include_tags,
                "exclude_tags": self.config.exclude_tags,
                "score_threshold": self.config.score_threshold,
                "must_have_check": self.config.must_have_check
            },
            "baseline_snapshot": self.baseline_snapshot.to_dict(),
            "variant_results": [result.to_dict() for result in self.variant_results],
            "regression_cases": [case.to_dict() for case in self.regression_cases],
            "comparison_report": self.comparison_report.to_dict() if self.comparison_report else None,
            "summary": self.summary,
            "created_at": self.created_at.isoformat()
        }


class RegressionTester:
    """å›å½’æµ‹è¯•æ‰§è¡Œå™¨"""
    
    def __init__(self, baseline_manager: Optional[BaselineManager] = None,
                 data_manager: Optional[DataManager] = None):
        """
        åˆå§‹åŒ–å›å½’æµ‹è¯•å™¨
        
        Args:
            baseline_manager: Baseline ç®¡ç†å™¨
            data_manager: æ•°æ®ç®¡ç†å™¨
        """
        self.baseline_manager = baseline_manager or BaselineManager()
        self.data_manager = data_manager or DataManager()
        self.testset_filter = TestsetFilter()
        
        # è¿›åº¦å›è°ƒ
        self.progress_callback: Optional[callable] = None
    
    def set_progress_callback(self, callback: callable):
        """è®¾ç½®è¿›åº¦å›è°ƒå‡½æ•°"""
        self.progress_callback = callback
    
    def run_regression_test(self, config: RegressionTestConfig) -> RegressionTestResult:
        """
        è¿è¡Œå›å½’æµ‹è¯•
        
        Args:
            config: å›å½’æµ‹è¯•é…ç½®
            
        Returns:
            å›å½’æµ‹è¯•ç»“æœ
        """
        # éªŒè¯é…ç½®
        config_errors = config.validate()
        if config_errors:
            raise ValueError(f"å›å½’æµ‹è¯•é…ç½®é”™è¯¯: {', '.join(config_errors)}")
        
        logger.info(f"å¼€å§‹å›å½’æµ‹è¯•: {config.entity_type}/{config.entity_id}")
        logger.info(f"Baseline: {config.baseline_name}")
        logger.info(f"å˜ä½“: {config.variant_name}")
        
        # åŠ è½½ baseline å¿«ç…§
        baseline_snapshot = self.baseline_manager.load_baseline(
            config.entity_type, config.entity_id, config.baseline_name
        )
        
        if not baseline_snapshot:
            raise ValueError(f"æœªæ‰¾åˆ° baseline: {config.baseline_name}")
        
        logger.info(f"å·²åŠ è½½ baseline å¿«ç…§ï¼Œåˆ›å»ºæ—¶é—´: {baseline_snapshot.created_at}")
        
        # åŠ è½½æµ‹è¯•é›†
        testset_samples = self._load_testset(config)
        logger.info(f"å·²åŠ è½½æµ‹è¯•é›†ï¼Œæ ·æœ¬æ•°é‡: {len(testset_samples)}")
        
        # æ‰§è¡Œå˜ä½“è¯„ä¼°
        if config.entity_type == "pipeline":
            variant_results = self._run_pipeline_variant_evaluation(config, testset_samples)
        else:
            variant_results = self._run_agent_variant_evaluation(config, testset_samples)
        
        logger.info(f"å˜ä½“è¯„ä¼°å®Œæˆï¼Œç»“æœæ•°é‡: {len(variant_results)}")
        
        # æ£€æµ‹å›å½’æ¡ˆä¾‹
        regression_cases = self._detect_regressions(
            baseline_snapshot, variant_results, config
        )
        
        logger.info(f"æ£€æµ‹åˆ° {len(regression_cases)} ä¸ªå›å½’æ¡ˆä¾‹")
        
        # ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š
        comparison_report = self._generate_comparison_report(
            baseline_snapshot, variant_results, regression_cases, config
        )
        
        # ç”Ÿæˆæ‘˜è¦
        summary = self._generate_regression_summary(
            baseline_snapshot, variant_results, regression_cases, config
        )
        
        result = RegressionTestResult(
            config=config,
            baseline_snapshot=baseline_snapshot,
            variant_results=variant_results,
            regression_cases=regression_cases,
            comparison_report=comparison_report,
            summary=summary
        )
        
        logger.info("å›å½’æµ‹è¯•å®Œæˆ")
        return result
    
    def _load_testset(self, config: RegressionTestConfig) -> List[Dict[str, Any]]:
        """åŠ è½½æµ‹è¯•é›†"""
        # ç¡®å®šæµ‹è¯•é›†è·¯å¾„
        if config.testset_path:
            testset_path = Path(config.testset_path)
        else:
            # ä½¿ç”¨é»˜è®¤æµ‹è¯•é›†è·¯å¾„
            testset_path = self.data_manager.find_testset_file(
                config.entity_type, config.entity_id, "regression.jsonl"
            )
            
            if not testset_path:
                # å°è¯•ä½¿ç”¨åŸºç¡€æµ‹è¯•é›†
                testset_path = self.data_manager.find_testset_file(
                    config.entity_type, config.entity_id, "base.jsonl"
                )
        
        if not testset_path or not testset_path.exists():
            raise FileNotFoundError(f"æœªæ‰¾åˆ°æµ‹è¯•é›†æ–‡ä»¶: {config.testset_path or 'regression.jsonl/base.jsonl'}")
        
        # åŠ è½½æ ·æœ¬
        samples = []
        with open(testset_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                
                try:
                    sample = json.loads(line)
                    samples.append(sample)
                except json.JSONDecodeError as e:
                    logger.warning(f"è·³è¿‡æ— æ•ˆçš„ JSON è¡Œ {line_num}: {e}")
        
        # åº”ç”¨æ ‡ç­¾è¿‡æ»¤
        if config.include_tags or config.exclude_tags:
            samples = self.testset_filter.filter_by_tags(
                samples, config.include_tags, config.exclude_tags
            )
        
        # åº”ç”¨æ ·æœ¬æ•°é‡é™åˆ¶
        if config.limit > 0:
            samples = samples[:config.limit]
        
        return samples
    
    def _run_pipeline_variant_evaluation(
        self, config: RegressionTestConfig, samples: List[Dict[str, Any]]
    ) -> List[EvaluationResult]:
        """è¿è¡Œ pipeline å˜ä½“è¯„ä¼°"""
        # åŠ è½½ pipeline é…ç½®
        pipeline_config = load_pipeline_config(config.entity_id)
        if not pipeline_config:
            raise ValueError(f"æœªæ‰¾åˆ° pipeline é…ç½®: {config.entity_id}")
        
        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = PipelineEvaluator(pipeline_config)
        if self.progress_callback:
            evaluator.set_progress_callback(self.progress_callback)
        
        # æ‰§è¡Œè¯„ä¼°
        evaluation_result = evaluator.evaluate_pipeline(
            samples=samples,
            variant=config.variant_name,
            apply_rules=config.apply_rules,
            apply_judge=config.apply_judge,
            limit=config.limit
        )
        
        return evaluation_result.sample_results
    
    def _run_agent_variant_evaluation(
        self, config: RegressionTestConfig, samples: List[Dict[str, Any]]
    ) -> List[EvaluationResult]:
        """è¿è¡Œ agent å˜ä½“è¯„ä¼°"""
        # TODO: å®ç° agent çº§åˆ«çš„å›å½’æµ‹è¯•
        # è¿™é‡Œéœ€è¦é›†æˆç°æœ‰çš„ agent è¯„ä¼°é€»è¾‘
        raise NotImplementedError("Agent çº§åˆ«çš„å›å½’æµ‹è¯•å°šæœªå®ç°")
    
    def _detect_regressions(
        self,
        baseline_snapshot: BaselineSnapshot,
        variant_results: List[EvaluationResult],
        config: RegressionTestConfig
    ) -> List[RegressionCase]:
        """æ£€æµ‹å›å½’æ¡ˆä¾‹"""
        regression_cases = []
        
        # ä» baseline å¿«ç…§ä¸­æå–è¯„ä¼°ç»“æœ
        baseline_results = {}
        for result_data in baseline_snapshot.evaluation_results:
            sample_id = result_data.get("sample_id", "")
            if sample_id:
                baseline_results[sample_id] = EvaluationResult.from_dict(result_data)
        
        # è®¡ç®—æ•´ä½“ç»Ÿè®¡ä¿¡æ¯ç”¨äºç›¸å¯¹é˜ˆå€¼åˆ¤æ–­
        baseline_scores = [r.overall_score for r in baseline_results.values() if r.overall_score > 0]
        variant_scores = [r.overall_score for r in variant_results if r.overall_score > 0]
        
        baseline_avg = sum(baseline_scores) / len(baseline_scores) if baseline_scores else 0.0
        variant_avg = sum(variant_scores) / len(variant_scores) if variant_scores else 0.0
        
        # è®¡ç®—åˆ†æ•°æ ‡å‡†å·®ç”¨äºç›¸å¯¹é˜ˆå€¼
        baseline_std = self._calculate_std(baseline_scores) if len(baseline_scores) > 1 else 0.0
        
        # æ¯”è¾ƒæ¯ä¸ªæ ·æœ¬çš„ç»“æœ
        for variant_result in variant_results:
            sample_id = variant_result.sample_id
            
            if sample_id not in baseline_results:
                continue
            
            baseline_result = baseline_results[sample_id]
            
            # æ£€æµ‹å„ç§ç±»å‹çš„å›å½’
            regression_info = self._analyze_sample_regression(
                baseline_result, variant_result, config, baseline_avg, baseline_std
            )
            
            if regression_info["is_regression"]:
                regression_case = RegressionCase(
                    sample_id=sample_id,
                    baseline_score=baseline_result.overall_score,
                    variant_score=variant_result.overall_score,
                    score_delta=regression_info["score_delta"],
                    severity=regression_info["severity"],
                    description=regression_info["description"]
                )
                regression_cases.append(regression_case)
        
        # åº”ç”¨é«˜çº§å›å½’æ£€æµ‹ç®—æ³•
        regression_cases = self._apply_advanced_regression_detection(
            regression_cases, baseline_results, variant_results, config
        )
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦å’Œåˆ†æ•°å˜åŒ–æ’åº
        regression_cases = self._prioritize_regression_cases(regression_cases)
        
        return regression_cases
    
    def _analyze_sample_regression(
        self,
        baseline_result: EvaluationResult,
        variant_result: EvaluationResult,
        config: RegressionTestConfig,
        baseline_avg: float,
        baseline_std: float
    ) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªæ ·æœ¬çš„å›å½’æƒ…å†µ"""
        score_delta = variant_result.overall_score - baseline_result.overall_score
        
        # æ£€æµ‹ must_have å›å½’
        must_have_regression = (
            config.must_have_check and
            baseline_result.must_have_pass and
            not variant_result.must_have_pass
        )
        
        # æ£€æµ‹æ–°çš„è§„åˆ™è¿è§„
        new_violations = set(variant_result.rule_violations) - set(baseline_result.rule_violations)
        
        # æ£€æµ‹æ‰§è¡Œæ—¶é—´æ˜¾è‘—å¢åŠ 
        execution_time_regression = False
        if (baseline_result.execution_time > 0 and variant_result.execution_time > 0):
            time_increase_ratio = variant_result.execution_time / baseline_result.execution_time
            execution_time_regression = time_increase_ratio > 2.0  # æ‰§è¡Œæ—¶é—´å¢åŠ è¶…è¿‡100%
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºå›å½’æ¡ˆä¾‹
        is_regression = False
        severity = "minor"
        description_parts = []
        
        # 1. ç»å¯¹åˆ†æ•°é˜ˆå€¼æ£€æµ‹
        if score_delta <= -config.score_threshold:
            is_regression = True
            if score_delta <= -2.0:
                severity = "critical"
            elif score_delta <= -1.0:
                severity = "major"
            else:
                severity = "minor"
            
            description_parts.append(f"åˆ†æ•°ä¸‹é™ {abs(score_delta):.2f}")
        
        # 2. ç›¸å¯¹åˆ†æ•°é˜ˆå€¼æ£€æµ‹ï¼ˆåŸºäºæ ‡å‡†å·®ï¼‰
        elif baseline_std > 0 and score_delta <= -2 * baseline_std:
            is_regression = True
            severity = "major" if score_delta <= -3 * baseline_std else "minor"
            description_parts.append(f"åˆ†æ•°æ˜¾è‘—ä¸‹é™ {abs(score_delta):.2f} (>{2 if severity == 'minor' else 3}Ïƒ)")
        
        # 3. Must-have å›å½’æ£€æµ‹
        if must_have_regression:
            is_regression = True
            severity = "critical"  # must_have å¤±è´¥æ€»æ˜¯ä¸¥é‡çš„
            description_parts.append("Must-have è¦æ±‚å¤±è´¥")
        
        # 4. æ–°è§„åˆ™è¿è§„æ£€æµ‹
        if new_violations:
            is_regression = True
            if severity == "minor":
                severity = "major"
            description_parts.append(f"æ–°å¢è§„åˆ™è¿è§„: {', '.join(new_violations)}")
        
        # 5. æ‰§è¡Œæ—¶é—´å›å½’æ£€æµ‹
        if execution_time_regression:
            is_regression = True
            if severity == "minor":
                severity = "major"
            time_increase = variant_result.execution_time - baseline_result.execution_time
            description_parts.append(f"æ‰§è¡Œæ—¶é—´æ˜¾è‘—å¢åŠ  {time_increase:.2f}s")
        
        # 6. é«˜åˆ†æ ·æœ¬çš„è½»å¾®ä¸‹é™ä¹Ÿè¦å…³æ³¨
        if (baseline_result.overall_score >= 8.0 and 
            score_delta <= -0.2 and 
            not is_regression):
            is_regression = True
            severity = "minor"
            description_parts.append(f"é«˜åˆ†æ ·æœ¬è½»å¾®ä¸‹é™ {abs(score_delta):.2f}")
        
        return {
            "is_regression": is_regression,
            "severity": severity,
            "score_delta": score_delta,
            "description": "; ".join(description_parts) if description_parts else "æœªçŸ¥å›å½’",
            "must_have_regression": must_have_regression,
            "new_violations": list(new_violations),
            "execution_time_regression": execution_time_regression
        }
    
    def _apply_advanced_regression_detection(
        self,
        regression_cases: List[RegressionCase],
        baseline_results: Dict[str, EvaluationResult],
        variant_results: List[EvaluationResult],
        config: RegressionTestConfig
    ) -> List[RegressionCase]:
        """åº”ç”¨é«˜çº§å›å½’æ£€æµ‹ç®—æ³•"""
        
        # 1. æ£€æµ‹ç³»ç»Ÿæ€§å›å½’æ¨¡å¼
        pattern_regressions = self._detect_systematic_regression_patterns(
            baseline_results, variant_results, config
        )
        
        # 2. æ£€æµ‹èšç±»å›å½’ï¼ˆç›¸ä¼¼æ ·æœ¬çš„é›†ä½“å›å½’ï¼‰
        cluster_regressions = self._detect_cluster_regressions(
            baseline_results, variant_results, config
        )
        
        # 3. æ£€æµ‹è¾¹ç•Œæ¡ˆä¾‹å›å½’
        edge_case_regressions = self._detect_edge_case_regressions(
            baseline_results, variant_results, config
        )
        
        # åˆå¹¶æ‰€æœ‰æ£€æµ‹åˆ°çš„å›å½’æ¡ˆä¾‹
        all_regressions = regression_cases + pattern_regressions + cluster_regressions + edge_case_regressions
        
        # å»é‡ï¼ˆåŸºäº sample_idï¼‰
        seen_samples = set()
        unique_regressions = []
        
        for regression in all_regressions:
            if regression.sample_id not in seen_samples:
                unique_regressions.append(regression)
                seen_samples.add(regression.sample_id)
            else:
                # å¦‚æœå·²å­˜åœ¨ï¼Œé€‰æ‹©æ›´ä¸¥é‡çš„ç‰ˆæœ¬
                for i, existing in enumerate(unique_regressions):
                    if existing.sample_id == regression.sample_id:
                        if self._compare_severity(regression.severity, existing.severity) > 0:
                            unique_regressions[i] = regression
                        break
        
        return unique_regressions
    
    def _detect_systematic_regression_patterns(
        self,
        baseline_results: Dict[str, EvaluationResult],
        variant_results: List[EvaluationResult],
        config: RegressionTestConfig
    ) -> List[RegressionCase]:
        """æ£€æµ‹ç³»ç»Ÿæ€§å›å½’æ¨¡å¼"""
        pattern_regressions = []
        
        # æ£€æµ‹æ•´ä½“æ€§èƒ½ä¸‹é™
        baseline_scores = [r.overall_score for r in baseline_results.values() if r.overall_score > 0]
        variant_scores = [r.overall_score for r in variant_results if r.overall_score > 0]
        
        if baseline_scores and variant_scores:
            baseline_avg = sum(baseline_scores) / len(baseline_scores)
            variant_avg = sum(variant_scores) / len(variant_scores)
            overall_delta = variant_avg - baseline_avg
            
            # å¦‚æœæ•´ä½“æ€§èƒ½ä¸‹é™è¶…è¿‡é˜ˆå€¼ï¼Œæ ‡è®°æ‰€æœ‰ä¸‹é™çš„æ ·æœ¬ä¸ºç³»ç»Ÿæ€§å›å½’
            if overall_delta <= -config.score_threshold:
                variant_map = {r.sample_id: r for r in variant_results}
                
                for sample_id, baseline_result in baseline_results.items():
                    if sample_id in variant_map:
                        variant_result = variant_map[sample_id]
                        sample_delta = variant_result.overall_score - baseline_result.overall_score
                        
                        if sample_delta < 0:  # åªæ ‡è®°ä¸‹é™çš„æ ·æœ¬
                            pattern_regressions.append(RegressionCase(
                                sample_id=sample_id,
                                baseline_score=baseline_result.overall_score,
                                variant_score=variant_result.overall_score,
                                score_delta=sample_delta,
                                severity="major",
                                description=f"ç³»ç»Ÿæ€§å›å½’ (æ•´ä½“ä¸‹é™ {abs(overall_delta):.2f})"
                            ))
        
        return pattern_regressions
    
    def _detect_cluster_regressions(
        self,
        baseline_results: Dict[str, EvaluationResult],
        variant_results: List[EvaluationResult],
        config: RegressionTestConfig
    ) -> List[RegressionCase]:
        """æ£€æµ‹èšç±»å›å½’ï¼ˆç›¸ä¼¼æ ·æœ¬çš„é›†ä½“å›å½’ï¼‰"""
        cluster_regressions = []
        
        # ç®€åŒ–çš„èšç±»æ£€æµ‹ï¼šåŸºäºè§„åˆ™è¿è§„æ¨¡å¼
        variant_map = {r.sample_id: r for r in variant_results}
        
        # æŒ‰è§„åˆ™è¿è§„ç±»å‹åˆ†ç»„
        violation_groups = {}
        for sample_id, baseline_result in baseline_results.items():
            if sample_id in variant_map:
                variant_result = variant_map[sample_id]
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„è¿è§„
                new_violations = set(variant_result.rule_violations) - set(baseline_result.rule_violations)
                
                if new_violations:
                    for violation in new_violations:
                        if violation not in violation_groups:
                            violation_groups[violation] = []
                        violation_groups[violation].append((sample_id, baseline_result, variant_result))
        
        # å¦‚æœæŸç§è¿è§„å½±å“äº†å¤šä¸ªæ ·æœ¬ï¼Œæ ‡è®°ä¸ºèšç±»å›å½’
        for violation, affected_samples in violation_groups.items():
            if len(affected_samples) >= 3:  # è‡³å°‘3ä¸ªæ ·æœ¬å—å½±å“
                for sample_id, baseline_result, variant_result in affected_samples:
                    score_delta = variant_result.overall_score - baseline_result.overall_score
                    
                    cluster_regressions.append(RegressionCase(
                        sample_id=sample_id,
                        baseline_score=baseline_result.overall_score,
                        variant_score=variant_result.overall_score,
                        score_delta=score_delta,
                        severity="major",
                        description=f"èšç±»å›å½’: {violation} (å½±å“ {len(affected_samples)} ä¸ªæ ·æœ¬)"
                    ))
        
        return cluster_regressions
    
    def _detect_edge_case_regressions(
        self,
        baseline_results: Dict[str, EvaluationResult],
        variant_results: List[EvaluationResult],
        config: RegressionTestConfig
    ) -> List[RegressionCase]:
        """æ£€æµ‹è¾¹ç•Œæ¡ˆä¾‹å›å½’"""
        edge_case_regressions = []
        
        variant_map = {r.sample_id: r for r in variant_results}
        
        # æ£€æµ‹åŸæœ¬è¡¨ç°å¾ˆå¥½ä½†ç°åœ¨å¤±è´¥çš„æ¡ˆä¾‹
        for sample_id, baseline_result in baseline_results.items():
            if sample_id in variant_map:
                variant_result = variant_map[sample_id]
                
                # é«˜åˆ†åˆ°ä½åˆ†çš„æ€¥å‰§ä¸‹é™
                if (baseline_result.overall_score >= 8.0 and 
                    variant_result.overall_score <= 5.0):
                    
                    score_delta = variant_result.overall_score - baseline_result.overall_score
                    edge_case_regressions.append(RegressionCase(
                        sample_id=sample_id,
                        baseline_score=baseline_result.overall_score,
                        variant_score=variant_result.overall_score,
                        score_delta=score_delta,
                        severity="critical",
                        description=f"è¾¹ç•Œæ¡ˆä¾‹å›å½’: é«˜åˆ†æ€¥å‰§ä¸‹é™ ({baseline_result.overall_score:.1f} â†’ {variant_result.overall_score:.1f})"
                    ))
                
                # ä»é€šè¿‡åˆ°å¤±è´¥çš„ must_have æ¡ˆä¾‹
                elif (baseline_result.must_have_pass and 
                      not variant_result.must_have_pass and
                      baseline_result.overall_score >= 7.0):
                    
                    score_delta = variant_result.overall_score - baseline_result.overall_score
                    edge_case_regressions.append(RegressionCase(
                        sample_id=sample_id,
                        baseline_score=baseline_result.overall_score,
                        variant_score=variant_result.overall_score,
                        score_delta=score_delta,
                        severity="critical",
                        description="è¾¹ç•Œæ¡ˆä¾‹å›å½’: é«˜è´¨é‡æ ·æœ¬ must_have å¤±è´¥"
                    ))
        
        return edge_case_regressions
    
    def _prioritize_regression_cases(self, regression_cases: List[RegressionCase]) -> List[RegressionCase]:
        """å¯¹å›å½’æ¡ˆä¾‹è¿›è¡Œä¼˜å…ˆçº§æ’åº"""
        def get_priority_score(case: RegressionCase) -> Tuple[int, float, float]:
            # ä¸¥é‡ç¨‹åº¦æƒé‡
            severity_weights = {"critical": 0, "major": 1, "minor": 2}
            severity_score = severity_weights.get(case.severity, 3)
            
            # åˆ†æ•°ä¸‹é™ç¨‹åº¦ï¼ˆè¶Šå¤§è¶Šä¸¥é‡ï¼‰
            score_impact = abs(case.score_delta)
            
            # åŸºçº¿åˆ†æ•°ï¼ˆåŸæœ¬è¶Šå¥½çš„æ ·æœ¬å›å½’è¶Šä¸¥é‡ï¼‰
            baseline_impact = -case.baseline_score  # è´Ÿå·ä½¿å¾—é«˜åˆ†æ’åœ¨å‰é¢
            
            return (severity_score, -score_impact, baseline_impact)
        
        return sorted(regression_cases, key=get_priority_score)
    
    def _compare_severity(self, severity1: str, severity2: str) -> int:
        """æ¯”è¾ƒä¸¤ä¸ªä¸¥é‡ç¨‹åº¦ï¼Œè¿”å› 1 å¦‚æœ severity1 æ›´ä¸¥é‡ï¼Œ-1 å¦‚æœ severity2 æ›´ä¸¥é‡ï¼Œ0 å¦‚æœç›¸ç­‰"""
        severity_order = {"critical": 3, "major": 2, "minor": 1}
        score1 = severity_order.get(severity1, 0)
        score2 = severity_order.get(severity2, 0)
        
        if score1 > score2:
            return 1
        elif score1 < score2:
            return -1
        else:
            return 0
    
    def _calculate_std(self, values: List[float]) -> float:
        """è®¡ç®—æ ‡å‡†å·®"""
        if len(values) <= 1:
            return 0.0
        
        mean = sum(values) / len(values)
        variance = sum((x - mean) ** 2 for x in values) / (len(values) - 1)
        return variance ** 0.5
    
    def _generate_comparison_report(
        self,
        baseline_snapshot: BaselineSnapshot,
        variant_results: List[EvaluationResult],
        regression_cases: List[RegressionCase],
        config: RegressionTestConfig
    ) -> ComparisonReport:
        """ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š"""
        # è®¡ç®—æ•´ä½“ç»Ÿè®¡
        total_samples = len(variant_results)
        
        # åˆ†æ•°ç»Ÿè®¡
        variant_scores = [r.overall_score for r in variant_results if r.overall_score > 0]
        avg_variant_score = sum(variant_scores) / len(variant_scores) if variant_scores else 0.0
        
        baseline_avg_score = baseline_snapshot.performance_metrics.get("average_score", 0.0)
        score_delta = avg_variant_score - baseline_avg_score
        
        # Must-have ç»Ÿè®¡
        variant_must_have_passed = len([r for r in variant_results if r.must_have_pass])
        variant_must_have_rate = variant_must_have_passed / total_samples if total_samples > 0 else 0.0
        
        baseline_must_have_rate = baseline_snapshot.performance_metrics.get("must_have_pass_rate", 0.0)
        must_have_delta = variant_must_have_rate - baseline_must_have_rate
        
        # è§„åˆ™è¿è§„ç»Ÿè®¡
        variant_rule_violations = {}
        for result in variant_results:
            for violation in result.rule_violations:
                variant_rule_violations[violation] = variant_rule_violations.get(violation, 0) + 1
        
        total_variant_violations = sum(variant_rule_violations.values())
        variant_violation_rate = total_variant_violations / total_samples if total_samples > 0 else 0.0
        
        baseline_violation_rate = baseline_snapshot.performance_metrics.get("rule_violation_rate", 0.0)
        rule_violation_delta = variant_violation_rate - baseline_violation_rate
        
        # æŒ‰æ ‡ç­¾åˆ†ææ€§èƒ½ï¼ˆå¦‚æœæœ‰æ ‡ç­¾ä¿¡æ¯ï¼‰
        tag_performance = {}
        # TODO: å®ç°æŒ‰æ ‡ç­¾çš„æ€§èƒ½åˆ†æ
        
        # é€‰æ‹©æœ€ä¸¥é‡çš„å›å½’æ¡ˆä¾‹
        worst_regressions = regression_cases[:10]  # å–å‰10ä¸ªæœ€ä¸¥é‡çš„
        
        # ç”Ÿæˆæ‘˜è¦
        summary_parts = []
        if score_delta < 0:
            summary_parts.append(f"å¹³å‡åˆ†æ•°ä¸‹é™ {abs(score_delta):.2f}")
        elif score_delta > 0:
            summary_parts.append(f"å¹³å‡åˆ†æ•°æå‡ {score_delta:.2f}")
        else:
            summary_parts.append("å¹³å‡åˆ†æ•°æ— å˜åŒ–")
        
        if must_have_delta < 0:
            summary_parts.append(f"Must-have é€šè¿‡ç‡ä¸‹é™ {abs(must_have_delta)*100:.1f}%")
        elif must_have_delta > 0:
            summary_parts.append(f"Must-have é€šè¿‡ç‡æå‡ {must_have_delta*100:.1f}%")
        
        if len(regression_cases) > 0:
            critical_count = len([c for c in regression_cases if c.severity == "critical"])
            major_count = len([c for c in regression_cases if c.severity == "major"])
            minor_count = len([c for c in regression_cases if c.severity == "minor"])
            
            summary_parts.append(f"å‘ç° {len(regression_cases)} ä¸ªå›å½’æ¡ˆä¾‹")
            if critical_count > 0:
                summary_parts.append(f"å…¶ä¸­ {critical_count} ä¸ªä¸¥é‡")
            if major_count > 0:
                summary_parts.append(f"{major_count} ä¸ªé‡è¦")
            if minor_count > 0:
                summary_parts.append(f"{minor_count} ä¸ªè½»å¾®")
        else:
            summary_parts.append("æœªå‘ç°å›å½’æ¡ˆä¾‹")
        
        summary = "ï¼›".join(summary_parts)
        
        return ComparisonReport(
            baseline_name=config.baseline_name,
            variant_name=config.variant_name,
            sample_count=total_samples,
            score_delta=score_delta,
            must_have_delta=must_have_delta,
            rule_violation_delta=rule_violation_delta,
            tag_performance=tag_performance,
            worst_regressions=worst_regressions,
            summary=summary
        )
    
    def _generate_regression_summary(
        self,
        baseline_snapshot: BaselineSnapshot,
        variant_results: List[EvaluationResult],
        regression_cases: List[RegressionCase],
        config: RegressionTestConfig
    ) -> str:
        """ç”Ÿæˆå›å½’æµ‹è¯•æ‘˜è¦"""
        summary_lines = [
            f"# å›å½’æµ‹è¯•æŠ¥å‘Š",
            f"",
            f"**å®ä½“**: {config.entity_type}/{config.entity_id}",
            f"**Baseline**: {config.baseline_name}",
            f"**å˜ä½“**: {config.variant_name}",
            f"**æµ‹è¯•æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"**æ ·æœ¬æ•°é‡**: {len(variant_results)}",
            f"",
            f"## æ•´ä½“ç»“æœ"
        ]
        
        # æ·»åŠ æ•´ä½“ç»Ÿè®¡
        if len(regression_cases) == 0:
            summary_lines.extend([
                f"âœ… **æœªå‘ç°å›å½’é—®é¢˜**",
                f"",
                f"æ‰€æœ‰æµ‹è¯•æ ·æœ¬çš„æ€§èƒ½éƒ½ç¬¦åˆé¢„æœŸï¼Œæ²¡æœ‰æ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ã€‚"
            ])
        else:
            critical_count = len([c for c in regression_cases if c.severity == "critical"])
            major_count = len([c for c in regression_cases if c.severity == "major"])
            minor_count = len([c for c in regression_cases if c.severity == "minor"])
            
            summary_lines.extend([
                f"âš ï¸ **å‘ç° {len(regression_cases)} ä¸ªå›å½’æ¡ˆä¾‹**",
                f"",
                f"- ä¸¥é‡å›å½’: {critical_count} ä¸ª",
                f"- é‡è¦å›å½’: {major_count} ä¸ª",
                f"- è½»å¾®å›å½’: {minor_count} ä¸ª",
                f""
            ])
            
            # æ·»åŠ æœ€ä¸¥é‡çš„å›å½’æ¡ˆä¾‹
            if critical_count > 0:
                critical_cases = [c for c in regression_cases if c.severity == "critical"][:5]
                summary_lines.extend([
                    f"### ä¸¥é‡å›å½’æ¡ˆä¾‹",
                    f""
                ])
                
                for case in critical_cases:
                    summary_lines.extend([
                        f"**æ ·æœ¬ {case.sample_id}**",
                        f"- åˆ†æ•°å˜åŒ–: {case.baseline_score:.2f} â†’ {case.variant_score:.2f} ({case.score_delta:+.2f})",
                        f"- é—®é¢˜æè¿°: {case.description}",
                        f""
                    ])
        
        # æ·»åŠ æ€§èƒ½å¯¹æ¯”
        baseline_metrics = baseline_snapshot.performance_metrics
        
        variant_scores = [r.overall_score for r in variant_results if r.overall_score > 0]
        variant_avg_score = sum(variant_scores) / len(variant_scores) if variant_scores else 0.0
        
        variant_must_have_passed = len([r for r in variant_results if r.must_have_pass])
        variant_must_have_rate = variant_must_have_passed / len(variant_results) if variant_results else 0.0
        
        summary_lines.extend([
            f"## æ€§èƒ½å¯¹æ¯”",
            f"",
            f"| æŒ‡æ ‡ | Baseline | å˜ä½“ | å˜åŒ– |",
            f"|------|----------|------|------|",
            f"| å¹³å‡åˆ†æ•° | {baseline_metrics.get('average_score', 0):.2f} | {variant_avg_score:.2f} | {variant_avg_score - baseline_metrics.get('average_score', 0):+.2f} |",
            f"| Must-Have é€šè¿‡ç‡ | {baseline_metrics.get('must_have_pass_rate', 0)*100:.1f}% | {variant_must_have_rate*100:.1f}% | {(variant_must_have_rate - baseline_metrics.get('must_have_pass_rate', 0))*100:+.1f}% |",
            f""
        ])
        
        # æ·»åŠ å»ºè®®
        if len(regression_cases) > 0:
            summary_lines.extend([
                f"## å»ºè®®",
                f""
            ])
            
            if critical_count > 0:
                summary_lines.append(f"- ğŸš¨ å‘ç°ä¸¥é‡å›å½’ï¼Œå»ºè®®ç«‹å³ä¿®å¤åå†å‘å¸ƒ")
            elif major_count > 0:
                summary_lines.append(f"- âš ï¸ å‘ç°é‡è¦å›å½’ï¼Œå»ºè®®è¯„ä¼°å½±å“åå†³å®šæ˜¯å¦å‘å¸ƒ")
            else:
                summary_lines.append(f"- â„¹ï¸ å‘ç°è½»å¾®å›å½’ï¼Œå¯è€ƒè™‘åœ¨åç»­ç‰ˆæœ¬ä¸­ä¼˜åŒ–")
            
            summary_lines.extend([
                f"- é‡ç‚¹å…³æ³¨å¤±è´¥çš„æ ·æœ¬ï¼Œåˆ†æå¤±è´¥åŸå› ",
                f"- è€ƒè™‘å¢åŠ é’ˆå¯¹æ€§çš„æµ‹è¯•ç”¨ä¾‹",
                f"- è¯„ä¼°æ˜¯å¦éœ€è¦è°ƒæ•´æ¨¡å‹å‚æ•°æˆ– prompt"
            ])
        else:
            summary_lines.extend([
                f"## å»ºè®®",
                f"",
                f"- âœ… å½“å‰å˜ä½“æ€§èƒ½è‰¯å¥½ï¼Œå¯ä»¥è€ƒè™‘å‘å¸ƒ",
                f"- å»ºè®®ç»§ç»­ç›‘æ§ç”Ÿäº§ç¯å¢ƒçš„æ€§èƒ½è¡¨ç°",
                f"- å¯ä»¥å°†å½“å‰å˜ä½“è®¾ç½®ä¸ºæ–°çš„ baseline"
            ])
        
        return "\n".join(summary_lines)
    
    def save_regression_test_result(
        self,
        result: RegressionTestResult,
        output_dir: Optional[Path] = None
    ) -> Dict[str, Path]:
        """
        ä¿å­˜å›å½’æµ‹è¯•ç»“æœ
        
        Args:
            result: å›å½’æµ‹è¯•ç»“æœ
            output_dir: è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„å­—å…¸
        """
        if not output_dir:
            output_dir = self.data_manager.get_entity_evals_dir(
                result.config.entity_type, result.config.entity_id
            ) / "regression"
        
        output_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = result.created_at.strftime("%Y-%m-%dT%H-%M-%S")
        base_filename = f"{result.config.entity_id}_{result.config.variant_name}_vs_{result.config.baseline_name}_{timestamp}"
        
        saved_files = {}
        
        # ä¿å­˜å®Œæ•´çš„ JSON ç»“æœ
        json_path = output_dir / f"{base_filename}.regression.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(result.to_dict(), f, ensure_ascii=False, indent=2)
        saved_files["json"] = json_path
        
        # ä¿å­˜æ‘˜è¦æŠ¥å‘Š
        summary_path = output_dir / f"{base_filename}.summary.md"
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write(result.summary)
        saved_files["summary"] = summary_path
        
        # ä¿å­˜å›å½’æ¡ˆä¾‹ CSV
        if result.regression_cases:
            import csv
            csv_path = output_dir / f"{base_filename}.regressions.csv"
            
            with open(csv_path, 'w', newline='', encoding='utf-8') as f:
                fieldnames = [
                    "sample_id", "baseline_score", "variant_score", 
                    "score_delta", "severity", "description"
                ]
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                
                for case in result.regression_cases:
                    writer.writerow(case.to_dict())
            
            saved_files["regressions_csv"] = csv_path
        
        logger.info(f"å›å½’æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
        return saved_files


# ä¾¿æ·å‡½æ•°
def run_pipeline_regression_test(
    pipeline_id: str,
    baseline_name: str,
    variant_name: str,
    testset_path: Optional[str] = None,
    include_tags: Optional[List[str]] = None,
    exclude_tags: Optional[List[str]] = None,
    score_threshold: float = 0.5,
    progress_callback: Optional[callable] = None
) -> RegressionTestResult:
    """è¿è¡Œ pipeline å›å½’æµ‹è¯•çš„ä¾¿æ·å‡½æ•°"""
    config = RegressionTestConfig(
        entity_type="pipeline",
        entity_id=pipeline_id,
        baseline_name=baseline_name,
        variant_name=variant_name,
        testset_path=testset_path,
        include_tags=include_tags or [],
        exclude_tags=exclude_tags or [],
        score_threshold=score_threshold
    )
    
    tester = RegressionTester()
    if progress_callback:
        tester.set_progress_callback(progress_callback)
    
    return tester.run_regression_test(config)


def run_agent_regression_test(
    agent_id: str,
    baseline_name: str,
    variant_name: str,
    testset_path: Optional[str] = None,
    include_tags: Optional[List[str]] = None,
    exclude_tags: Optional[List[str]] = None,
    score_threshold: float = 0.5,
    progress_callback: Optional[callable] = None
) -> RegressionTestResult:
    """è¿è¡Œ agent å›å½’æµ‹è¯•çš„ä¾¿æ·å‡½æ•°"""
    config = RegressionTestConfig(
        entity_type="agent",
        entity_id=agent_id,
        baseline_name=baseline_name,
        variant_name=variant_name,
        testset_path=testset_path,
        include_tags=include_tags or [],
        exclude_tags=exclude_tags or [],
        score_threshold=score_threshold
    )
    
    tester = RegressionTester()
    if progress_callback:
        tester.set_progress_callback(progress_callback)
    
    return tester.run_regression_test(config)


class RegressionAnalyzer:
    """å›å½’åˆ†æå™¨ - æä¾›æ·±åº¦åˆ†æå’Œæ´å¯Ÿ"""
    
    def __init__(self):
        """åˆå§‹åŒ–å›å½’åˆ†æå™¨"""
        pass
    
    def analyze_regression_patterns(
        self,
        regression_cases: List[RegressionCase],
        baseline_results: Dict[str, EvaluationResult],
        variant_results: List[EvaluationResult]
    ) -> Dict[str, Any]:
        """
        åˆ†æå›å½’æ¨¡å¼å’Œè¶‹åŠ¿
        
        Args:
            regression_cases: å›å½’æ¡ˆä¾‹åˆ—è¡¨
            baseline_results: åŸºçº¿ç»“æœå­—å…¸
            variant_results: å˜ä½“ç»“æœåˆ—è¡¨
            
        Returns:
            å›å½’æ¨¡å¼åˆ†æç»“æœ
        """
        variant_map = {r.sample_id: r for r in variant_results}
        
        analysis = {
            "severity_distribution": self._analyze_severity_distribution(regression_cases),
            "score_impact_analysis": self._analyze_score_impact(regression_cases),
            "failure_pattern_analysis": self._analyze_failure_patterns(
                regression_cases, baseline_results, variant_map
            ),
            "temporal_analysis": self._analyze_temporal_patterns(regression_cases),
            "root_cause_analysis": self._analyze_root_causes(
                regression_cases, baseline_results, variant_map
            ),
            "recovery_recommendations": self._generate_recovery_recommendations(
                regression_cases, baseline_results, variant_map
            )
        }
        
        return analysis
    
    def _analyze_severity_distribution(self, regression_cases: List[RegressionCase]) -> Dict[str, Any]:
        """åˆ†æä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ"""
        severity_counts = {"critical": 0, "major": 0, "minor": 0}
        severity_scores = {"critical": [], "major": [], "minor": []}
        
        for case in regression_cases:
            severity_counts[case.severity] += 1
            severity_scores[case.severity].append(abs(case.score_delta))
        
        total = len(regression_cases)
        
        return {
            "counts": severity_counts,
            "percentages": {
                severity: count / total * 100 if total > 0 else 0
                for severity, count in severity_counts.items()
            },
            "average_impact": {
                severity: sum(scores) / len(scores) if scores else 0
                for severity, scores in severity_scores.items()
            },
            "max_impact": {
                severity: max(scores) if scores else 0
                for severity, scores in severity_scores.items()
            }
        }
    
    def _analyze_score_impact(self, regression_cases: List[RegressionCase]) -> Dict[str, Any]:
        """åˆ†æåˆ†æ•°å½±å“"""
        score_deltas = [case.score_delta for case in regression_cases]
        
        if not score_deltas:
            return {"error": "æ²¡æœ‰å›å½’æ¡ˆä¾‹"}
        
        score_deltas.sort()
        n = len(score_deltas)
        
        # åˆ†æ•°ä¸‹é™åŒºé—´åˆ†æ
        impact_ranges = {
            "è½»å¾® (0-0.5)": len([d for d in score_deltas if -0.5 <= d < 0]),
            "ä¸­ç­‰ (0.5-1.0)": len([d for d in score_deltas if -1.0 <= d < -0.5]),
            "ä¸¥é‡ (1.0-2.0)": len([d for d in score_deltas if -2.0 <= d < -1.0]),
            "æä¸¥é‡ (>2.0)": len([d for d in score_deltas if d < -2.0])
        }
        
        return {
            "total_cases": n,
            "average_impact": sum(score_deltas) / n,
            "median_impact": score_deltas[n // 2] if n % 2 == 1 else (score_deltas[n // 2 - 1] + score_deltas[n // 2]) / 2,
            "worst_impact": min(score_deltas),
            "impact_ranges": impact_ranges,
            "cumulative_impact": sum(abs(d) for d in score_deltas)
        }
    
    def _analyze_failure_patterns(
        self,
        regression_cases: List[RegressionCase],
        baseline_results: Dict[str, EvaluationResult],
        variant_map: Dict[str, EvaluationResult]
    ) -> Dict[str, Any]:
        """åˆ†æå¤±è´¥æ¨¡å¼"""
        patterns = {
            "must_have_failures": [],
            "rule_violation_patterns": {},
            "score_drop_patterns": {},
            "execution_time_issues": []
        }
        
        for case in regression_cases:
            sample_id = case.sample_id
            
            if sample_id in baseline_results and sample_id in variant_map:
                baseline = baseline_results[sample_id]
                variant = variant_map[sample_id]
                
                # Must-have å¤±è´¥åˆ†æ
                if baseline.must_have_pass and not variant.must_have_pass:
                    patterns["must_have_failures"].append({
                        "sample_id": sample_id,
                        "baseline_score": baseline.overall_score,
                        "variant_score": variant.overall_score,
                        "new_violations": list(set(variant.rule_violations) - set(baseline.rule_violations))
                    })
                
                # è§„åˆ™è¿è§„æ¨¡å¼åˆ†æ
                new_violations = set(variant.rule_violations) - set(baseline.rule_violations)
                for violation in new_violations:
                    if violation not in patterns["rule_violation_patterns"]:
                        patterns["rule_violation_patterns"][violation] = []
                    patterns["rule_violation_patterns"][violation].append(sample_id)
                
                # åˆ†æ•°ä¸‹é™æ¨¡å¼åˆ†æ
                score_range = self._get_score_range(baseline.overall_score)
                if score_range not in patterns["score_drop_patterns"]:
                    patterns["score_drop_patterns"][score_range] = []
                patterns["score_drop_patterns"][score_range].append({
                    "sample_id": sample_id,
                    "baseline_score": baseline.overall_score,
                    "variant_score": variant.overall_score,
                    "delta": case.score_delta
                })
                
                # æ‰§è¡Œæ—¶é—´é—®é¢˜åˆ†æ
                if (baseline.execution_time > 0 and variant.execution_time > 0 and
                    variant.execution_time > baseline.execution_time * 1.5):
                    patterns["execution_time_issues"].append({
                        "sample_id": sample_id,
                        "baseline_time": baseline.execution_time,
                        "variant_time": variant.execution_time,
                        "increase_ratio": variant.execution_time / baseline.execution_time
                    })
        
        return patterns
    
    def _analyze_temporal_patterns(self, regression_cases: List[RegressionCase]) -> Dict[str, Any]:
        """åˆ†ææ—¶é—´æ¨¡å¼ï¼ˆå¦‚æœæœ‰æ—¶é—´æˆ³ä¿¡æ¯ï¼‰"""
        # è¿™é‡Œå¯ä»¥åˆ†æå›å½’æ¡ˆä¾‹æ˜¯å¦æœ‰æ—¶é—´ç›¸å…³çš„æ¨¡å¼
        # ç›®å‰ç®€åŒ–å®ç°ï¼Œä¸»è¦åˆ†æä¸¥é‡ç¨‹åº¦çš„åˆ†å¸ƒè¶‹åŠ¿
        
        severity_timeline = []
        for i, case in enumerate(regression_cases):
            severity_timeline.append({
                "index": i,
                "severity": case.severity,
                "impact": abs(case.score_delta)
            })
        
        return {
            "severity_timeline": severity_timeline,
            "trend_analysis": self._analyze_severity_trend(severity_timeline)
        }
    
    def _analyze_severity_trend(self, timeline: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æä¸¥é‡ç¨‹åº¦è¶‹åŠ¿"""
        if len(timeline) < 3:
            return {"trend": "insufficient_data"}
        
        # ç®€å•çš„è¶‹åŠ¿åˆ†æï¼šçœ‹ä¸¥é‡ç¨‹åº¦æ˜¯å¦æœ‰èšé›†
        severity_weights = {"critical": 3, "major": 2, "minor": 1}
        
        first_half = timeline[:len(timeline)//2]
        second_half = timeline[len(timeline)//2:]
        
        first_avg = sum(severity_weights[item["severity"]] for item in first_half) / len(first_half)
        second_avg = sum(severity_weights[item["severity"]] for item in second_half) / len(second_half)
        
        if second_avg > first_avg * 1.2:
            trend = "worsening"
        elif second_avg < first_avg * 0.8:
            trend = "improving"
        else:
            trend = "stable"
        
        return {
            "trend": trend,
            "first_half_severity": first_avg,
            "second_half_severity": second_avg
        }
    
    def _analyze_root_causes(
        self,
        regression_cases: List[RegressionCase],
        baseline_results: Dict[str, EvaluationResult],
        variant_map: Dict[str, EvaluationResult]
    ) -> Dict[str, Any]:
        """åˆ†ææ ¹æœ¬åŸå› """
        root_causes = {
            "prompt_related": [],
            "model_related": [],
            "logic_related": [],
            "data_related": [],
            "unknown": []
        }
        
        for case in regression_cases:
            sample_id = case.sample_id
            
            if sample_id in baseline_results and sample_id in variant_map:
                baseline = baseline_results[sample_id]
                variant = variant_map[sample_id]
                
                # åŸºäºæè¿°å’Œè¿è§„ç±»å‹æ¨æ–­æ ¹æœ¬åŸå› 
                cause_category = self._infer_root_cause(case, baseline, variant)
                root_causes[cause_category].append({
                    "sample_id": sample_id,
                    "severity": case.severity,
                    "description": case.description,
                    "evidence": self._collect_evidence(case, baseline, variant)
                })
        
        return root_causes
    
    def _infer_root_cause(
        self,
        case: RegressionCase,
        baseline: EvaluationResult,
        variant: EvaluationResult
    ) -> str:
        """æ¨æ–­æ ¹æœ¬åŸå› ç±»åˆ«"""
        description = case.description.lower()
        new_violations = set(variant.rule_violations) - set(baseline.rule_violations)
        
        # åŸºäºè§„åˆ™è¿è§„ç±»å‹æ¨æ–­
        if any("æ ¼å¼" in v or "ç»“æ„" in v for v in new_violations):
            return "prompt_related"
        elif any("é€»è¾‘" in v or "æ¨ç†" in v for v in new_violations):
            return "logic_related"
        elif any("æ•°æ®" in v or "ä¿¡æ¯" in v for v in new_violations):
            return "data_related"
        elif "æ‰§è¡Œæ—¶é—´" in description:
            return "model_related"
        elif "must_have" in description:
            return "logic_related"
        else:
            return "unknown"
    
    def _collect_evidence(
        self,
        case: RegressionCase,
        baseline: EvaluationResult,
        variant: EvaluationResult
    ) -> Dict[str, Any]:
        """æ”¶é›†è¯æ®ä¿¡æ¯"""
        return {
            "score_change": case.score_delta,
            "must_have_change": baseline.must_have_pass != variant.must_have_pass,
            "new_violations": list(set(variant.rule_violations) - set(baseline.rule_violations)),
            "removed_violations": list(set(baseline.rule_violations) - set(variant.rule_violations)),
            "execution_time_change": variant.execution_time - baseline.execution_time if baseline.execution_time > 0 else 0
        }
    
    def _generate_recovery_recommendations(
        self,
        regression_cases: List[RegressionCase],
        baseline_results: Dict[str, EvaluationResult],
        variant_map: Dict[str, EvaluationResult]
    ) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ¢å¤å»ºè®®"""
        recommendations = []
        
        # åˆ†æä¸»è¦é—®é¢˜ç±»å‹
        severity_counts = {"critical": 0, "major": 0, "minor": 0}
        must_have_failures = 0
        rule_violations = {}
        
        for case in regression_cases:
            severity_counts[case.severity] += 1
            
            sample_id = case.sample_id
            if sample_id in baseline_results and sample_id in variant_map:
                baseline = baseline_results[sample_id]
                variant = variant_map[sample_id]
                
                if baseline.must_have_pass and not variant.must_have_pass:
                    must_have_failures += 1
                
                new_violations = set(variant.rule_violations) - set(baseline.rule_violations)
                for violation in new_violations:
                    rule_violations[violation] = rule_violations.get(violation, 0) + 1
        
        # åŸºäºåˆ†æç»“æœç”Ÿæˆå»ºè®®
        if severity_counts["critical"] > 0:
            recommendations.append({
                "priority": "high",
                "category": "immediate_action",
                "title": "ç«‹å³å¤„ç†ä¸¥é‡å›å½’",
                "description": f"å‘ç° {severity_counts['critical']} ä¸ªä¸¥é‡å›å½’æ¡ˆä¾‹ï¼Œå»ºè®®ç«‹å³åœæ­¢å‘å¸ƒå¹¶ä¿®å¤",
                "actions": [
                    "å›æ»šåˆ°ä¸Šä¸€ä¸ªç¨³å®šç‰ˆæœ¬",
                    "åˆ†æä¸¥é‡å›å½’æ¡ˆä¾‹çš„å…±åŒç‰¹å¾",
                    "ä¿®å¤æ ¹æœ¬é—®é¢˜åé‡æ–°æµ‹è¯•"
                ]
            })
        
        if must_have_failures > 0:
            recommendations.append({
                "priority": "high",
                "category": "must_have_fixes",
                "title": "ä¿®å¤ Must-Have è¦æ±‚å¤±è´¥",
                "description": f"{must_have_failures} ä¸ªæ ·æœ¬çš„ must_have è¦æ±‚å¤±è´¥",
                "actions": [
                    "æ£€æŸ¥ prompt æ˜¯å¦æ­£ç¡®å¤„ç†å¿…è¦æ¡ä»¶",
                    "éªŒè¯æ¨¡å‹å‚æ•°è®¾ç½®",
                    "å¢åŠ é’ˆå¯¹æ€§çš„æµ‹è¯•ç”¨ä¾‹"
                ]
            })
        
        if rule_violations:
            top_violations = sorted(rule_violations.items(), key=lambda x: x[1], reverse=True)[:3]
            recommendations.append({
                "priority": "medium",
                "category": "rule_compliance",
                "title": "æ”¹å–„è§„åˆ™åˆè§„æ€§",
                "description": f"ä¸»è¦è¿è§„ç±»å‹: {', '.join([v[0] for v in top_violations])}",
                "actions": [
                    f"é‡ç‚¹å…³æ³¨ {top_violations[0][0]} è¿è§„ ({top_violations[0][1]} æ¬¡)",
                    "æ£€æŸ¥ç›¸å…³çš„ prompt æŒ‡ä»¤",
                    "è€ƒè™‘è°ƒæ•´è§„åˆ™é˜ˆå€¼æˆ–æ¨¡å‹å‚æ•°"
                ]
            })
        
        if severity_counts["minor"] > severity_counts["critical"] + severity_counts["major"]:
            recommendations.append({
                "priority": "low",
                "category": "optimization",
                "title": "æ€§èƒ½ä¼˜åŒ–å»ºè®®",
                "description": "ä¸»è¦æ˜¯è½»å¾®å›å½’ï¼Œå¯ä»¥é€šè¿‡ä¼˜åŒ–æ”¹å–„",
                "actions": [
                    "åˆ†æè½»å¾®å›å½’çš„æ¨¡å¼",
                    "è€ƒè™‘ prompt å¾®è°ƒ",
                    "å¢åŠ æ›´å¤šè®­ç»ƒæ•°æ®"
                ]
            })
        
        return recommendations
    
    def _get_score_range(self, score: float) -> str:
        """è·å–åˆ†æ•°èŒƒå›´æ ‡ç­¾"""
        if score >= 9.0:
            return "ä¼˜ç§€ (9.0-10.0)"
        elif score >= 8.0:
            return "è‰¯å¥½ (8.0-9.0)"
        elif score >= 7.0:
            return "ä¸­ç­‰ (7.0-8.0)"
        elif score >= 6.0:
            return "åŠæ ¼ (6.0-7.0)"
        else:
            return "ä¸åŠæ ¼ (<6.0)"
    
    def generate_detailed_analysis_report(
        self,
        regression_cases: List[RegressionCase],
        baseline_results: Dict[str, EvaluationResult],
        variant_results: List[EvaluationResult],
        config: RegressionTestConfig
    ) -> str:
        """ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Š"""
        analysis = self.analyze_regression_patterns(
            regression_cases, 
            {r.sample_id: r for r in baseline_results.values()}, 
            variant_results
        )
        
        report_lines = [
            f"# è¯¦ç»†å›å½’åˆ†ææŠ¥å‘Š",
            f"",
            f"**å®ä½“**: {config.entity_type}/{config.entity_id}",
            f"**Baseline**: {config.baseline_name}",
            f"**å˜ä½“**: {config.variant_name}",
            f"**åˆ†ææ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"",
            f"## å›å½’ä¸¥é‡ç¨‹åº¦åˆ†æ"
        ]
        
        severity_dist = analysis["severity_distribution"]
        report_lines.extend([
            f"",
            f"| ä¸¥é‡ç¨‹åº¦ | æ•°é‡ | å æ¯” | å¹³å‡å½±å“ | æœ€å¤§å½±å“ |",
            f"|----------|------|------|----------|----------|",
            f"| ä¸¥é‡ | {severity_dist['counts']['critical']} | {severity_dist['percentages']['critical']:.1f}% | {severity_dist['average_impact']['critical']:.2f} | {severity_dist['max_impact']['critical']:.2f} |",
            f"| é‡è¦ | {severity_dist['counts']['major']} | {severity_dist['percentages']['major']:.1f}% | {severity_dist['average_impact']['major']:.2f} | {severity_dist['max_impact']['major']:.2f} |",
            f"| è½»å¾® | {severity_dist['counts']['minor']} | {severity_dist['percentages']['minor']:.1f}% | {severity_dist['average_impact']['minor']:.2f} | {severity_dist['max_impact']['minor']:.2f} |",
            f""
        ])
        
        # åˆ†æ•°å½±å“åˆ†æ
        score_impact = analysis["score_impact_analysis"]
        if "error" not in score_impact:
            report_lines.extend([
                f"## åˆ†æ•°å½±å“åˆ†æ",
                f"",
                f"- **æ€»å›å½’æ¡ˆä¾‹**: {score_impact['total_cases']}",
                f"- **å¹³å‡å½±å“**: {score_impact['average_impact']:.2f}",
                f"- **ä¸­ä½æ•°å½±å“**: {score_impact['median_impact']:.2f}",
                f"- **æœ€ä¸¥é‡å½±å“**: {score_impact['worst_impact']:.2f}",
                f"- **ç´¯è®¡å½±å“**: {score_impact['cumulative_impact']:.2f}",
                f"",
                f"### å½±å“ç¨‹åº¦åˆ†å¸ƒ",
                f""
            ])
            
            for range_name, count in score_impact["impact_ranges"].items():
                percentage = count / score_impact['total_cases'] * 100 if score_impact['total_cases'] > 0 else 0
                report_lines.append(f"- {range_name}: {count} ä¸ª ({percentage:.1f}%)")
            
            report_lines.append("")
        
        # å¤±è´¥æ¨¡å¼åˆ†æ
        failure_patterns = analysis["failure_pattern_analysis"]
        
        if failure_patterns["must_have_failures"]:
            report_lines.extend([
                f"## Must-Have å¤±è´¥åˆ†æ",
                f"",
                f"å‘ç° {len(failure_patterns['must_have_failures'])} ä¸ª must_have å¤±è´¥æ¡ˆä¾‹:",
                f""
            ])
            
            for failure in failure_patterns["must_have_failures"][:5]:  # æ˜¾ç¤ºå‰5ä¸ª
                report_lines.extend([
                    f"**æ ·æœ¬ {failure['sample_id']}**",
                    f"- åˆ†æ•°å˜åŒ–: {failure['baseline_score']:.2f} â†’ {failure['variant_score']:.2f}",
                    f"- æ–°å¢è¿è§„: {', '.join(failure['new_violations']) if failure['new_violations'] else 'æ— '}",
                    f""
                ])
        
        if failure_patterns["rule_violation_patterns"]:
            report_lines.extend([
                f"## è§„åˆ™è¿è§„æ¨¡å¼åˆ†æ",
                f""
            ])
            
            for violation, samples in failure_patterns["rule_violation_patterns"].items():
                report_lines.append(f"**{violation}**: å½±å“ {len(samples)} ä¸ªæ ·æœ¬")
            
            report_lines.append("")
        
        # æ ¹æœ¬åŸå› åˆ†æ
        root_causes = analysis["root_cause_analysis"]
        report_lines.extend([
            f"## æ ¹æœ¬åŸå› åˆ†æ",
            f""
        ])
        
        for cause_type, cases in root_causes.items():
            if cases:
                cause_names = {
                    "prompt_related": "Prompt ç›¸å…³",
                    "model_related": "æ¨¡å‹ç›¸å…³", 
                    "logic_related": "é€»è¾‘ç›¸å…³",
                    "data_related": "æ•°æ®ç›¸å…³",
                    "unknown": "æœªçŸ¥åŸå› "
                }
                report_lines.append(f"**{cause_names.get(cause_type, cause_type)}**: {len(cases)} ä¸ªæ¡ˆä¾‹")
        
        report_lines.append("")
        
        # æ¢å¤å»ºè®®
        recommendations = analysis["recovery_recommendations"]
        if recommendations:
            report_lines.extend([
                f"## æ¢å¤å»ºè®®",
                f""
            ])
            
            for rec in recommendations:
                priority_emoji = {"high": "ğŸš¨", "medium": "âš ï¸", "low": "â„¹ï¸"}
                report_lines.extend([
                    f"### {priority_emoji.get(rec['priority'], 'â€¢')} {rec['title']}",
                    f"",
                    f"{rec['description']}",
                    f"",
                    f"**å»ºè®®è¡ŒåŠ¨**:",
                    f""
                ])
                
                for action in rec["actions"]:
                    report_lines.append(f"- {action}")
                
                report_lines.append("")
        
        return "\n".join(report_lines)


# ä¾¿æ·å‡½æ•°æ‰©å±•
def analyze_regression_results(
    regression_result: RegressionTestResult
) -> Dict[str, Any]:
    """åˆ†æå›å½’æµ‹è¯•ç»“æœçš„ä¾¿æ·å‡½æ•°"""
    analyzer = RegressionAnalyzer()
    
    baseline_results = {}
    for result_data in regression_result.baseline_snapshot.evaluation_results:
        sample_id = result_data.get("sample_id", "")
        if sample_id:
            baseline_results[sample_id] = EvaluationResult.from_dict(result_data)
    
    return analyzer.analyze_regression_patterns(
        regression_result.regression_cases,
        baseline_results,
        regression_result.variant_results
    )


def generate_regression_analysis_report(
    regression_result: RegressionTestResult
) -> str:
    """ç”Ÿæˆå›å½’åˆ†ææŠ¥å‘Šçš„ä¾¿æ·å‡½æ•°"""
    analyzer = RegressionAnalyzer()
    
    baseline_results = {}
    for result_data in regression_result.baseline_snapshot.evaluation_results:
        sample_id = result_data.get("sample_id", "")
        if sample_id:
            baseline_results[sample_id] = EvaluationResult.from_dict(result_data)
    
    return analyzer.generate_detailed_analysis_report(
        regression_result.regression_cases,
        baseline_results,
        regression_result.variant_results,
        regression_result.config
    )